\section{Evaluation}
\label{sec:evaluation}

% TODO: if this table is after tab:case-studies, then it doesn't appear? WTF


% a line in tab:case-studies
% arguments: project name, original LoC, # of resources (-AcountMustCall), diff size, # of annotations, TPs, Confirmed TPs, FPs, run time in seconds
\newcommand{\osstablerow}[8]{\textbf{\smaller{#1}} & #2 & #3 & #4 & #5 & #6 & #7 & #8}

\begin{table*}
  \caption{Verifying the absence of resource leaks in case studies.
    % LoC is computed using scc. Be careful when computing LoC to run scc from relevant top-level
    % directory that only contains source code (not test code!) to make sure that the count is accurate.
    % for Zookeeper, this directory is zookeeper/zookeeper-server/src/main/java
    Throughout, ``LoC'' is lines of non-comment, non-blank Java code.
    ``Resources'' is the number of resources created by the program.
    % to compute the diff size, do something like the following (this is what I did for Zookeeper) from the with-annotations branch:
    % > git diff origin/with-checker -- '*.java'
    % then count the number of individual changesets and report that
    ``Annos.'' is number of manually-written annotations to specify
    existing methods.
    ``Code changes'' is the number of distinct changes program text we made,
    not including changes that will be erased at compile time
    (such as annotations or warning suppressions).
    ``TPs'' is true positives.
    ``FPs'' is false positives, where the our analysis could not
  guarantee that the call was safe, but manual analysis revealed that no
  run-time failure was possible.
  % ``RT(s)'' is the wall-clock run time of our analysis.
  }
  \label{tab:case-studies}

  \begin{tabular}{@{}lrr|rr|rr|r@{}}
    Project                                    &      LoC      & Resources   &  Annos.  &  Code changes   & TPs      & FPs & Wall-clock time      \\
    \hline
    \osstablerow{apache/zookeeper:zookeeper-server} {45,410}        {232}       {99}          {5}       {10}       {47}   {1m 24s}        \\
    \osstablerow{apache/hadoop:hdfs}                   {151,233}        {366}            {171}          {?}        {21}        {36}   {16m 21s}        \\
    \osstablerow{apache/hbase:server}            {170,569}        {62}            {45}          {1}        {11}        {21}   {7m 45s}        \\
    \hline
    \osstablerow{\textbf{Total}}                        {563,855}        {660}            {315}          {?}        {42}       {104}   {-}        \\
  \end{tabular}
\end{table*}


Our evaluation has three parts:
\begin{itemize}
\item case studies on open-source projects, which show that our approach
  is scalable and finds real bugs (\cref{sec:case-studies}).
\item an evaluation of the contributions of our enhancements:
  lightweight ownership, % (\cref{sec:lightweight-ownership}),
  resource aliasing, % (\cref{sec:must-call-choice}), and
  accumulation frames % (\cref{sec:reset-must-call})
  (\cref{sec:ablation}).
\item a comparison to previous leak detectors:  both heuristic bug-finding
  and heavy-weight whole-program
  % typestate
  analysis (\cref{sec:compare}).
\end{itemize}

\subsection{Case studies on open-source projects}
\label{sec:case-studies}

We selected \todo{3} popular open-source projects with significant
usage of resources that were analyzed by prior work~\cite{zuo2019grapple}.
For each project, we selected and analyzed one module
containing significant uses of leakable resources. We used
the latest version of the source code that was available
when we analyzed the code.

We modified the
build system to run our analysis on the module. We manually
annotated each program with must-call, called-methods, and ownership
annotations (the ``Annos'' column in \cref{tab:case-studies}).
We also
made small, semantics-preserving changes to the programs---such as adding
\<final> modifiers to fields---to reduce
false positives from our analysis. We report the total number
of such changes to each project, excluding changes
that have no impact on run-time behavior, such as annotations, comments,
warning suppressions, or import statements (``Code changes'').
The annotations and code changes are discussed further in \cref{sec:annos}.
We ran the analysis and manually categorized each warning as revealing a
real resource leak (a true positive or TP) or as correct code that our
system is unable to prove correct (a false positive or FP\@).
We discarded all warnings concerning uses of classes that are not
part of the JDK.
The
annotated versions of the case study programs (including
the commit hashes that we analyzed) are available at
\todo{an anonymized fork of the projects}.
While performing the case study, most of our time was spent
understanding the code under analysis (which no authors were familiar
with before beginning), writing annotations and especially categorizing
warnings as true or false positives.

We also measured the number of
possible resource leaks in each project (``resources''),
by instrumenting our analysis to record each time that
a new obligation was introduced for an object whose declared type
is part of the JDK.
We measured the run time,
as the median of 5 trials on
% This is Mike's home machine.  Are there any other relevant parameters?
a machine with an Intel Core i7-10700 CPU running at 2.90GHz and 64GiB of RAM\@.


\Cref{tab:case-studies} summarizes the results. \Tool found multiple
serious resource leak bugs in every program. \Tool issues
more false positives than true positives in each program, but
the number is small enough to be examined by a single developer in a
few hours.  This is a small price to pay for knowing that the program is
definitely free of resource leaks.  The annotations in the program are
also a benefit: as a form of machine-checked documentation, they
express the programmer's intent and, unlike traditional comments,
cannot become out-of-date if the checker is passing.

\subsubsection{True and false positive examples}
\label{sec:examples}

This section describes some examples of warnings reported by \Tool
in our case studies.\looseness=-1

\begin{figure}
  \lstinputlisting{hadoop-bug.txt}
  \caption{A true positive that we found in Hadoop. Our pull request
    to fix this bug was accepted by Hadoop's developers.}
  \label{fig:hadoop-bug}
\end{figure}

\Cref{fig:hadoop-bug} contains code from Hadoop. If an IO error
occurs any time between the allocation of the \<FileInputStream>
in the first line of the method and the \<return> statement
at the end---for example, if \<wrapInputStreamForCompression>
throws an \<IOException>, as it is specified to do---then the
only reference to the stream is lost. Hadoop's developers
assigned this bug a priorty of ``Major'' and accepted our patch
\footnoteanonurl{https://github.com/apache/hadoop/pull/2652}.
One developer suggested using a try-with-resources statement instead
of our patch (which catches the exception and closes the stream),
but we pointed out that that was impossible in this case, since
the file needs to remain open if no error occurs so that it can be
returned.

\begin{figure}
  \lstinputlisting{zookeeper-optional.txt}
  \caption{Code from the ZooKeeper case study that causes our tool
  to issue a false positive.}
  \label{fig:zookeeper-optional}
\end{figure}

Most false positives were caused by unique coding patterns that
each needed to be verified by hand; for example, a series of catch
statements, each of which set an \<error> boolean to \<true>, followed
by a \<finally> statement that closed the relevant socket if \<error>
was true---\Tool reports an error because it cannot reason about arbitrarily
complex path conditions. There were some similar patterns of false positives:
for example, across all three benchmarks the most common (with 15 instances) false
positive was caused by
a generic container object like \<java.util.Optional> taking ownership of a resource, such
as the example in \cref{fig:zookeeper-optional}. Our lightweight ownership
system does not support transferring ownership to generic parameters,
so we issue an error when \<Optional.of> is returned. In this case, the use
of the \<Optional> class complicates the code; if \<Optional> was replaced
by \<null>,
%% Leave out of submitted version.
% as some advocate~\cite{ErnstNothingIsBetterThanOptional},
\Tool could verify this code. We leave expanding the lightweight ownership system to
support Java generics as future work.

\subsubsection{Annotations and code changes}
\label{sec:annos}

\begin{table}
  \caption{The number of annotations that we wrote, across all
    three case studies.\todo{Order these by how common they are, once
    we have final counts.}}
  \label{tab:annos}
  % counts @InheritableMustCall as @MustCall, for simplicity of presentation
  \begin{tabularx}{\columnwidth}{@{}Xr@{}}
    Annotation          &      Count     \\
    \hline
    \<@Owning>          &      42+      \\
    \<@NotOwning>       &      13+       \\
    \<@MustCallAlias>   &      16+       \\
    \<@ResetMustCall>   &      21+      \\
    \<@EnsuresCalledMethods> & 25+      \\
    \<@MustCall>        &      23+      \\
    \<@PolyMustCall>    &      4+       \\
  \end{tabularx}
\end{table}

Because most relevant \<@MustCall> and \<@CalledMethods> annotations
are inferred intra-procedurally, most annotations that we had to write in
the case studies pertained to ownership or the side-effects of procedures.
\Cref{tab:annos} shows how many of each annotation we wrote, across all three
case studies.

\begin{table}
  \caption{The reasons we made code changes to the
    case study programs.\todo{Order these by how common they are, once
    we have final counts.}}
  \label{tab:changes}
  \begin{tabularx}{\columnwidth}{@{}Xr@{}}
    Reason                            &      Count     \\
    \hline
    add \<final> to a field           &     3   \\
    re-order \<try> and null-check    &     2   \\
    extract method calls into local   &     1   \\
  \end{tabularx}
\end{table}

When we made changes to the programs, they were small (the largest was \todo{4} lines)
and localized. \Cref{tab:changes} gives the reasons that changes were made.
\todo{These sentences must be in the same order as the table.}
%
Adding \<final> to a field allows our checker to verify it without using
accumulation frames, which have stricter requirements.
%
Re-ordering a \<try>
statement and a null-check makes reasoning more precise by removing an
infeasible control-flow-graph edge.
%
Extracting multiple calls to a method
into a single call that is assigned to a local variable allows the local
variable to be reasoned about flow-sensitively.

\subsection{Evaluating our enhancements}
% \subsection{Ablating lightweight ownership, resource aliasing, and accumulation frames}
\label{sec:ablation}

% a line in tab:ablation
% note that the FULL column is always zero, so it's not included here
% arguments: project name, no-LO, no-RA, no-AF
\newcommand{\abltablerow}[4]{\textbf{\smaller{#1}} & #2 & #3 & #4}

\begin{table}
  \caption{The contribution of the lightweight
    ownership, resource aliasing,
    and accumulation frames
    features in reducing false positives. Each entry is the number of extra
    false positive warnings reported by the variant with the given feature disabled on the given project.}
  \label{tab:ablation}
  
  \begin{tabularx}{\columnwidth}{@{}Xrrr@{}}
    Project                              &      without LO & without RA & without AF     \\
    \hline
    \abltablerow{apache/zookeeper}              {66}            {97}             {10}                               \\
    \abltablerow{apache/hadoop}                   {?}            {?}             {?}                               \\
    \abltablerow{apache/hbase}                  {52}            {57}             {-2}                               \\
    \hline
    \abltablerow{\textbf{Total}}                {?}            {?}             {?}                               \\
  \end{tabularx}
\end{table}

The base analysis of \cref{sec:base-type-systems} produces significantly
more false positives, because it lacks 
lightweight ownership (\cref{sec:lightweight-ownership}),
resource aliasing (\cref{sec:must-call-choice}), and
accumulation frames (\cref{sec:reset-must-call}).
each contribute to \Tool's precision by eliminating false positives.
To show the contribution of each enhancement, we individually disabled each
feature and re-ran the experiments in \cref{sec:case-studies}.
Since all variants are sound (no false
negatives), any difference in warnings is a false positive that is prevented
by the feature.

\Cref{tab:ablation} shows that each of lightweight
ownership and resource aliases prevents more false positive errors than the total number
of remaining false positives on the Zookeeper and HBase benchmarks, for example. \todo{Strengthen
  this claim when we have the results from the other benchmarks.} Though
the accumulation frame feature prevents fewer errors (and even adds two more errors
on HBase, which includes only two \ResetMustCall annotations), it permits verification
of a complex pattern---the use of non-final, owning fields---that is used in a
few critical places in each benchmark.

\subsection{Comparison to other tools}
\label{sec:compare}

Our approach represents a novel point in the design space of resource leak checkers.

In this section, we compare our approach with two other modern tools that detect resource leaks:
\begin{itemize}
\item The analysis built into the Eclipse Compiler for Java (ecj), which is the default approach
  for detecting resource leaks in the Eclipse IDE~\cite{ecj-resource-leak}.
\item Grapple~\cite{zuo2019grapple}, an unsound, typestate-based analysis that represents a significant, recent
  improvement in the scalability of typestate-based tools that require a whole-program alias analysis.
\end{itemize}
Each tool is evaluated against the four criteria that we proposed in \cref{sec:intro}
for an ideal resource-leak detector: analysis speed, applicability to legacy code, soundness (i.e. ability
to detect all errors), and precision (i.e. lack of false positive warnings).

\subsubsection{Eclipse}
\label{sec:eclipse}

The Eclipse analysis is a simple dataflow analysis
augmented with heuristics. Because it is tightly integrated with
the compiler, it scales well and runs quickly. It contains
heuristics for ownership, resource wrappers, and resource-free
closeables, among others; these are all hard-coded into the analysis and cannot
be adjusted by the user. It does not support annotations to express
specifications that differ from its defaults.
It supports two levels of analysis: detecting high-confidence resource
leaks and detecting ``potential'' resource
leaks (a superset of high-confidence resource leaks).

We ran this analysis on the same version of Zookeeper's zookeeper-server
module that we ran \Tool on in \cref{sec:case-studies}. It is easy to apply
to legacy code and it is fast---nearly instantaneous once Eclipse
has loaded the project.

In ``high-confidence'' mode on zookeeper-server, Eclipse reports 3
warnings: 1 true positive (thus, it misses 9 real resource leaks) and 2
false positives.
In ``potential'' leak mode, the analysis reported 180
warnings:  3 true positives (it misses 7 real resource leaks) and 177 false
positives.
The most common cause of false
positives was the unchangeable, default ownership transfer assumption
at method invocations: each call to a method such as
\<Socket\#getInputStream> that returns a resource-alias resulted in a
warning.

These results demonstrate that the Eclipse bug-finder is unsound in both
modes: it misses at least 7 real warnings that our tool found when detecting
potential leaks. Further, the analysis is imprecise: in potential leak mode,
the vast majority of warnings are false positives. However, this sort
of bug-finding tool does have two advantages: it is fast, and it is easily
applicable to legacy code. Compared to our tool, the Eclipse analysis
is much faster and equally applicable to legacy code, but both less sound
and less precise.

%% Chandra rightly pointed out that this para make us look lazy. Let's just
%% say what we did, not why we did it.
%%
%% Given that these results on zookeeper-server
%% clearly show the strengths and weaknesses of this tool compared to our own,
%% we did not run it on our other case study programs.

\subsubsection{Grapple}
\label{sec:grapple}

% a line in tab:grapple
% these numbers come from tables 2 and 3 in the grapple paper from EuroSys.
% for TPs and FPs, I added together the IO and Socket columns of table 2.
\newcommand{\grappletablerow}[4]{\textbf{\smaller{#1}} & #2 & #3 & #4}

\begin{table}
  \caption{Key findings on the Grapple tool's performance on the case study
    programs in \cref{sec:case-studies}; reproduced from~\cite{zuo2019grapple}.}
  \label{tab:grapple}
  
  \begin{tabularx}{\columnwidth}{@{}Xrrr@{}}
    Project                              &  TPs    &    FPs         & Run time      \\
    \hline
    \grappletablerow{ZooKeeper}             {6}         {0}           {01h 07m 02s}     \\
    \grappletablerow{HDFS}                  {5}         {2}           {01h 54m 52s}    \\
    \grappletablerow{HBase}                 {15}        {2}           {33h 51m 59s}     \\
    \hline
    \grappletablerow{\textbf{Total}}        {26}        {4}           {-}          \\
  \end{tabularx}
\end{table}

\todo{Be very sure that Grapple is unsound - read their paper again, and
   point out why in this paragraph.}
\todo{be sure to note that Grapple is not modular, unlike us}
Grapple~\cite{zuo2019grapple} is a modern typestate-based resource leak analysis
focused on precision and (relative) scalability. Grapple models its alias and
dataflow analyses as dynamic transitive-closure computation of graph
representations of the target programs. Grapple, like all previous
typestate-based analyses, requires a whole-program alias analysis. Grapple
contains four checkers, of which two (IO and Socket) are useful for detecting
resource leaks.

The Grapple authors have already evaluated their tool on earlier
versions\todo{Why don't we repeat our experiments on those earlier
  versions?  This should be easy to do since it's only a few hours of work
  and we are now familiar with the programs.}
 of the case study programs in
\cref{sec:case-studies}~\cite{zuo2019grapple}.
\Cref{tab:grapple} reproduces their claimed results.
Unfortunately, we were
neither able to reproduce their results, nor to run Grapple on current
versions of the programs.  Although the Grapple framework is publicly
available, the analysis is not, and the Grapple authors did not provide it
to us when we requested it.  We also requested Grapple's output, so that we
could compare it to our tool, but the Grapple authors did not provide that
to us either.

Note that the TP and FP numbers not perfectly comparable between a modular
and a whole-program analysis.
Our tool reports violations of a user-supplied specification
(which takes effort to write but provides documentation benefits), so it
can ensure that a library is correct for all possible clients.  By
contrast, Grapple checks a library in the context of one specific client.

Our approach is complementary to Grapple's.
Compared to our approach, Grapple is unsound, 
much more precise,
% equally applicable to legacy code,
and much slower (due to its reliance on
an expensive whole-program alias analysis).
Different users can select whichever tool matches
their priorities.


% LocalWords:  LoC SHAs leakable
