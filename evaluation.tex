\section{Evaluation}
\label{sec:evaluation}

Our evaluation has three parts:
\begin{itemize}
\item case studies on open-source projects, which show that our approach
  is scalable and finds real resource leaks (\cref{sec:case-studies}).
\item an evaluation of the importance of
  lightweight ownership, % (\cref{sec:lightweight-ownership}),
  resource aliasing, % (\cref{sec:must-call-choice}), and
  and obligation creation % (\cref{sec:reset-must-call})
  (\cref{sec:ablation}).
\item a comparison to previous leak detectors:  both a heuristic bug finder
  and a whole-program
  % typestate
  analysis (\cref{sec:compare}).
\end{itemize}

% \noindent
% \paragraph{Results}

All code and data for our experiments described
in this section, including \tool's implementation, experimental
machinery, and annotated versions of our case study programs,
are publicly available at \url{https://doi.org/10.5281/zenodo.4902321}.

\subsection{Case Studies on Open-Source Projects}
\label{sec:case-studies}

We selected 3 open-source projects that were analyzed by prior work~\cite{zuo2019grapple}.
For each, we selected and analyzed one or two modules
with many uses of leakable resources. We used
the latest version of the source code that was available
when we began. We also analyzed an open-source project
maintained by one of the authors, to simulate \tool's
expected use case, where the user is already familiar with the
code under analysis (see \cref{sec:plume-util}).

For each case study, our methodology was as follows.
(1)
We modified the
build system to run \tool on the module(s), analyzing uses of resource
classes that are defined in the JDK\@.
It also reports the maximum possible number of resources (references to JDK-defined
classes with a non-empty \<@MustCall> obligation) that could be
leaked:  each obligation at a formal parameter or method call.
(2) We manually
annotated each program with must-call, called-methods, and ownership
annotations (see \cref{sec:annos}).
% the ``Annos'' column in \cref{tab:case-studies}).
(3) We iteratively ran the analysis to correct our annotations.
We measured the run time
as the median of 5 trials on
% This is Mike's home machine.  Are there any other relevant parameters?
a machine running Ubuntu 20.04 with an Intel Core i7-10700 CPU running at 2.90GHz and 64GiB of RAM\@.
Our analysis is embarrassingly parallel, but our implementation is
single-threaded because javac is single-threaded.
(4) We manually categorized each warning as revealing a
real resource leak (a true positive) or as a
false positive warning about safe code that our tool is unable to prove correct.
At least two authors agreed on each categorization.

\Cref{tab:case-studies} summarizes the results. \Tool found multiple
serious resource leaks in every program. \Tool's overall precision on
these case studies is 29\% (49/170). Though there are more false positives
than true positives,
the number is small enough to be examined by a single developer in a
few hours.
The annotations in the program are
also a benefit: they
express the programmer's intent and, as machine-checked documentation,
they cannot become out-of-date.\looseness=-1

% \todo{does this para need to be updated for plume-util?} MK: No - I have already done so and the plume-lib fixes are included in the 16 number.
%
% It is important that the developers be the actors in the following sentence, because we have other patches under review but not yet accepted.
At the time of writing,
the developers of the case study programs have validated and accepted
patches for 16 resource leaks discovered by our tool, including at least
one for each project.
% \todo{do we need the next sentence?} MK: IMO we do, because otherwise an uncharitable reader might say that our precision is actually 16/166 instead
% of 49/166, because only 16 of the 49 were validated (because they assumed that the rest were rejected).
No patches we have submitted this way have been rejected.

\subsubsection{True and False Positive Examples}
\label{sec:examples}

This section gives examples of warnings reported by \tool.

\begin{figure}
  \lstinputlisting{hadoop-bug.txt}
  \prefigcaption
  \caption{A resource leak that \tool found in Hadoop. Hadoop's developers
    merged our fix~\cite{hdfs-15791}.}
  \label{fig:hadoop-bug}
\end{figure}

\Cref{fig:hadoop-bug} contains code from Hadoop. If an IO error
occurs any time between the allocation of the \<FileInputStream>
in the first line of the method and the \<return> statement
at the end---for example, if \<channel.position(section.getOffset())>
throws an \<IOException>, as it is specified to do---then the
only reference to the stream is lost. Hadoop's developers
assigned this issue a priority of ``Major'' and accepted our
patch~\cite{hdfs-15791}.
One developer suggested using a try-with-resources statement instead
of our patch (which catches the exception and closes the stream),
but we pointed out that
the file needs to remain open if no error occurs so that it can be
returned.

\begin{figure}
  \lstinputlisting{zookeeper-optional.txt}
  \prefigcaption
  \caption{Code from the ZooKeeper case study that causes \tool
  to issue a false positive.}
  \label{fig:zookeeper-optional}
\end{figure}

The most common reason for false
positives (which caused 22\% of the false positives in our case studies) was
a known bug in the Checker Framework's
type inference algorithm for Java generics, which the Checker Framework
developers are working to fix~\cite{issue979}.
The second most common reason (causing 15\%)
was a generic container object like \<java.util.Optional> taking ownership of a resource, such
as the example in \cref{fig:zookeeper-optional}. Our lightweight ownership
system does not support transferring ownership to generic parameters,
so \tool issues an error when \<Optional.of> is returned. In this case, the use
of the \<Optional> class is unnecessary and complicates the
code~\cite{ErnstNothingIsBetterThanOptional}.  If \<Optional> was replaced
by a nullable Java reference,
\tool could verify this code. Future work should expand the lightweight ownership system to
support Java generics.
The third most common reason (causing 8\%) is nullness reasoning: some resource
is closed only if it is non-null, but our checker expects the resource to be
closed on every path. Our checker handles simple comparisons with
\<null> (as in \cref{fig:example}), but future work could
incorporate more complex nullness reasoning~\cite{PapiACPE2008}.\looseness=-1

% Most of the remaining false positives involve unique coding patterns.
% One example\todo{It's weird to give a random reason.  Replace this by the
%   third most common pattern, and state its prevalence as a percentage.
%   Readers will see the percentages and know
%   that all the other reasons are in the noise.} \todo{MK: I like this example for explanatory
%   purposes, so I added the third most common reason but left this in. If you really don't like it,
%   Mike, (or we badly need space) I don't object to removing it and the accompanying figure.}
%  is a series of catch
% statements that each sets an \<error> boolean to \<true>, and
% a \<finally> statement that closes the relevant socket if \<error>
% was true.  \Tool reports an error because it does not reason about path
% conditions.


\subsubsection{Annotations and Code Changes}\label{sec:annotations-and-code-changes}
\label{sec:annos}

\input{table-annotations.tex}

% Because most relevant \MustCall and \<@CalledMethods> annotations
% are inferred intra-procedurally, most annotations that we had to write in
% the case studies pertained to ownership or the side-effects of procedures.
We wrote about one annotation per 1,500 lines of code (\cref{tab:annos}).

We also
made 42 small, semantics-preserving changes to the programs to reduce
false positives from our analysis.
%
In 19 places in plume-util, we added an explicit \<extends> bound to a generic type.
The Checker Framework uses different defaulting rules for implicit and explicit
upper bounds, and a common pattern in this benchmark caused our checker to issue
an error on uses of implicit bounds.
%
In 18 places, we made a field \<final>; this allows our checker to verify usage
of the field without using the stricter rules for non-final owning fields given in \cref{sec:reset-must-call}.
In 9 of those cases, we also removed assignments of \<null> to the field after it was closed; in 1 other we added an \<else> clause in the constructor that assigned the field
a \<null> value.
%
In 3 places, we re-ordered two statements to remove an
infeasible control-flow-graph edge.
% In 2 places, we re-ordered a \<try> statement and a null-check.
% % In 1 place, we re-ordered two calls to \<close()> that closed
% the same resource (via resource aliases). The first call in the
% unmodified code was conditional, but the second was
% not, leading to an infeasible control-flow graph edge.
%
In 2 places, we extracted an expression into a local variable, permitting
flow-sensitive reasoning or targeting by an \CreatesMustCallFor annotation.
% multiple calls to a method
% into a single call that is assigned to a local variable; this allows
% flow-sensitive reasoning about the local
% variable.
% % , and the code is also more efficient.
% In 1 place, we extracted an expression into a local variable
% so that a \CreatesMustCallFor annotation could target it.

\subsubsection{Simulating the User Experience}
\label{sec:plume-util}

To simulate the experience of a typical user who understands
the codebase being analyzed,
one author used \tool to analyze plume-util,
a 10KLoC library he wrote 23 years ago.
The process took about two hours, including running the tool,
writing annotations, and fixing the 8 resource leaks that the tool discovered.
The annotations were valuable enough that they
are now committed to that codebase, and \tool runs in CI
to prevent the introduction of new resource leaks.
This example is suggestive that the programmer effort to use our tool is reasonable.


\subsection{Evaluating Our Enhancements}
\label{sec:ablation}

% a line in tab:ablation
% arguments: project name, no-LO + total, no-RA + total, no-AF + total, and total FP count
\newcommand{\abltablerow}[5]{\textbf{\smaller{#1}} & #2 & #3 & #4 & #5}

\begin{table}
  \caption{False positives in our case studies (``RLC'') and
    without
    %% Cut for space.
    % the
    lightweight
    ownership (``w/o LO''), resource aliasing (``w/o RA''),
    and obligation creation (``w/o OC'').
    %% Cut for space.
    % features.
  }
  \label{tab:ablation}
  \posttablecaption
  
  \begin{tabularx}{\columnwidth}{@{}Xrrrr@{}}
    Project                              &    w/o LO & w/o RA & w/o OC & RLC     \\
    \hline
    \abltablerow{apache/zookeeper}              {117}            {158}             {54}      {48}                         \\
    \abltablerow{apache/hadoop}                   {97}            {184}             {52}    {49}                           \\
    \abltablerow{apache/hbase}                  {82}            {93}             {26}       {22}                        \\
    \abltablerow{plume-lib/plume-util}          {4}                {11}             {3}         {2}                    \\
    \hline
    \abltablerow{\textbf{Total}}                {300}            {446}             {135}    {121}                           \\
  \end{tabularx}
\end{table}

Lightweight ownership (\cref{sec:lightweight-ownership}),
resource aliasing (\cref{sec:must-call-choice}), and
obligation creation (\cref{sec:reset-must-call})
reduce false positive warnings and improve \tool's precision.
To evaluate the contribution of each enhancement, we individually disabled each
feature and re-ran the experiments of \cref{sec:case-studies}.

\Cref{tab:ablation} shows that each of lightweight
ownership and resource aliases prevents more false positive warnings than the total number
of remaining false positives on each benchmark.
The system for creating new obligations at points other than constructors reduces
false positives by a smaller amount: non-final, owning field re-assignments
are rare.
% \todo{I suggest cutting the rest of this sentence.  It doesn't add
%   much and we don't give that information for any other features.  Most
%   importantly, mentioning this makes it sound like our set of features is
%   ad hoc and we chose them just to make our tool perform well on this
%   particular set of benchmarks.},
% and we encountered the unconnected socket pattern described in \cref{sec:unconnected-sockets}
% only in ZooKeeper.


\subsection{Comparison to Other Tools}
\label{sec:compare}

Our approach represents a novel point in the design space of resource leak checkers.
%
This section compares our approach with two other modern tools that detect resource leaks:
\begin{itemize}
\item The analysis built into the Eclipse Compiler for Java (ecj), which is the default approach
  for detecting resource leaks in the Eclipse IDE~\cite{ecj-resource-leak}.
  We used version 4.18.0.
\item Grapple~\cite{zuo2019grapple}, a state-of-the-art typestate checker that
leverages whole-program alias analysis.
\end{itemize}
In brief, both of the above tools are unsound and missed 87--93\% of leaks.
Both tools neither require nor permit user-written specifications,
a plus in terms of ease of use but a minus in terms of documentation and
flexibility. Eclipse is very fast (nearly instantaneous) but has low precision
(25\% for high-confidence warnings, \emph{much} lower if all warnings are
included). Grapple is more precise (50\% precision), but
an order of
magnitude slower than \tool.  \Tool had 100\% recall and 26\%
precision.  Users can select whichever tool matches their priorities.

\Cref{tab:tool-comparison,tab:run-times} quantitatively compare the
tools.  Our comparison uses parts of the 3 case study programs that Grapple was
run on in the past; see \cref{sec:grapple} for details.

% Gets the (two-column) Grapple table on the same page as the reference to it.
\input{table-grapple.tex}


\subsubsection{Eclipse}
\label{sec:eclipse}

The Eclipse analysis is a simple dataflow analysis
augmented with heuristics. Since it is tightly integrated with
the compiler, it scales well and runs quickly. It has
heuristics for ownership, resource wrappers, and resource-free
closeables, among others; these are all hard-coded into the analysis and cannot
be adjusted by the user.
%% Repetitive.
% It does not support annotations to express
% specifications that differ from its defaults.
It supports two levels of analysis: detecting high-confidence resource
leaks and detecting ``potential'' resource
leaks (a superset of high-confidence resource leaks).\looseness=-1

We ran Eclipse's analysis on the exact same code
that we ran \tool on for \cref{sec:case-studies} (excluding the plume-util case study).
\Cref{tab:tool-comparison} reports results for a subset of the code; this
paragraph reports results for the full code.
% we already said this --Manu
% It is easy to apply
% to legacy code and it is fast---nearly instantaneous once Eclipse
% has loaded the project.\looseness=-1
In ``high-confidence'' mode on the three projects, Eclipse reports 8
warnings related to classes defined in the JDK:
2 true positives (thus, it misses 39 real resource leaks) and 6
false positives.
In ``potential'' leak mode, the analysis reports many more warnings.
Thus, we triaged only the 180
warnings about JDK classes from the ZooKeeper benchmark.
Among these were 3 true positives (it missed 10 real resource leaks) and 177 false
positives (2\% precision).
The most common cause of false
positives was the unchangeable, default ownership transfer assumption
at method returns, leading to a warning at each call that returns a resource-alias, such as
\<Socket\#getInputStream>.

%% This is redundant.
% These results show that the Eclipse bug-finder is unsound in both
% modes: it misses 10 real warnings on ZooKeeper even in potential leak mode.
% The analysis is imprecise, too: in potential leak mode,
% the vast majority of warnings are false positives, and even in high-confidence
% mode its precision is 25\%, roughly the same as \tool.
%% MK: I think this sentence is redundant
%% However, this sort
%% of bug-finding tool does have two advantages: it is fast, and it is easily
%% applicable to legacy code.


%% Chandra rightly pointed out that this para make us look lazy. Let's just
%% say what we did, not why we did it.
%%
%% Given that these results on zookeeper-server
%% clearly show the strengths and weaknesses of this tool compared to our own,
%% we did not run it on our other case study programs.


\subsubsection{Grapple}
\label{sec:grapple}

Grapple is a modern typestate-based resource leak analysis
``designed to conduct precise and scalable checking of finite-state properties for very
large codebases''~\cite{zuo2019grapple}. Grapple models its alias and
dataflow analyses as dynamic transitive-closure computations over graphs, and
it leverages novel path encodings and techniques from predecessor-system
Graspan~\cite{wang2017graspan} to achieve both context- and path-sensitivity.  
% Grapple, like all previous
% typestate-based analyses, requires a whole-program alias analysis.
Grapple contains four checkers, of which two can detect
resource leaks.  Unlike \tool, Grapple is unsound, as it performs a fixed bounded unrolling
of loops to make path sensitivity tractable.
\Tool reports violations of a user-supplied specification
(which takes effort to write but provides documentation benefits), so it
can ensure that a library is correct for all possible clients.  By
contrast, Grapple checks a library in the context of one specific client; it
only reports issues in methods reachable from entry points (like a \<main()>
method) in a whole-program call graph~\cite{grapplepersonal}.\looseness=-1

The Grapple authors evaluated their tool on earlier versions of the first three
case study programs in \cref{sec:case-studies}~\cite{zuo2019grapple}.
Unfortunately, a direct comparison on our benchmark versions is not possible, because
Grapple's leak detector currently cannot be run
% Mike feels this parenthesis is essential.  Otherwise, there is no support
% for the claim "cannot be run".  The fact that the Grapple authors cannot run it
% is essential information.
(by us or by the Grapple authors) due to library incompatibilities and bitrot in
the implementation. The Grapple authors provided us with the finite-state
machine (FSM) specifications used in Grapple to detect resource leaks, and also
details of all warnings issued by Grapple in the versions of the benchmarks they
analyzed.

We used the following methodology to permit a head-to-head comparison.
We started with all warnings issued by either tool.
We disregarded any warning about code that is not present identically in the other
version of the target program (due to refactoring, added code,
bug fixes, etc.).  We also disregarded warnings about code that
is not checked by both tools.  For example, Grapple analyzed test
code, but our experiments did not write annotations in test code nor
type-check it.  The remaining warnings pertain to resource leaks in identical code that both tools ought to report.
For each remaining warning, we manually identified it as a true positive (a
real resource leak) or a false positive (correct code, but the tool cannot
determine that fact).  \Cref{tab:tool-comparison} reports the precision
and recall of Eclipse, Grapple, and \tool.  Some of
Grapple's false positives are reports about types like
\<java.io.StringWriter> with no underlying resource that must be
closed.
(These reports were mis-classified as true positives in~\cite{zuo2019grapple},
which is one reason the numbers there differ from \cref{tab:tool-comparison}.)
Grapple's false negatives might be due to analysis unsoundness or gaps in API
modeling (e.g., Grapple does not include FSM specifications for \<OutputStream>
classes).
%% This doesn't really fit in the paragraph, which didn't directly compare
%% the tools until this sentence.
% On the flip side, \tool has lower precision (more false positives)
% than Grapple, and it requires effort to annotate the source code.  

\begin{table}
  \caption{Run times of resource leak checking tools.}
  \label{tab:run-times}
  \posttablecaption
  \begin{tabular}{l|ccc}
    Project        & Eclipse & Grapple & Resource Leak Checker \\
    \hline
    ZooKeeper      & <5s & \zph 1h 07m 02s  & \zph 1m 24s  \\
    HDFS           & <5s & \zph 1h 54m 52s  &  16m 21s \\
    HBase          & <5s &     33h 51m 59s  & \zph 7m 45s  \\
  \end{tabular}
\end{table}

Grapple runs can take many hours (run times are from~\cite{zuo2019grapple}), whereas
\tool runs in minutes (\cref{tab:run-times}).
Further, Grapple is not modular, so if the user edits their program, Grapple must be
re-run from scratch~\cite{grapplepersonal}.  After a code edit,
\tool only needs to re-analyze modified code (and
possibly its dependents if the modified code's interface changed).

% LocalWords:  LoC SHAs leakable Closeable i7 GiB hadoop FileInputStream
% LocalWords:  getOffset ZooKeeper util apache hbase ecj getInputStream OC
% LocalWords:  codebases'' Graspan HBase HDFS MustCall nullable KLoC RLC
% LocalWords:  bitrot mis OutputStream
