\section{Evaluation}

% TODO: if this table is after tab:case-studies, then it doesn't appear? WTF

% a line in tab:ablation
% note that the FULL column is always zero, so it's not included here
% arguments: project name, NONE, LO-ONLY, AF-ONLY
\newcommand{\abltablerow}[4]{\textbf{\smaller{#1}} & #2 & #3 & #4 & 0}

\begin{table}
  \caption{An ablation study on the contribution of the lightweight
    ownership (\emph{NO-LO}), resource aliasing (\emph{NO-RA}),
    and accumulation frames (\emph{NO-AF})
    features in reducing false positives. Each entry is the number of extra
    false positive warnings reported by the variant with the given feature disabled on the given project.}
  \label{tab:ablation}
  
  \begin{tabularx}{\columnwidth}{@{}Xrrrr@{}}
    Project                              &      \emph{NO-LO} & \emph{NO-RA} & \emph{NO-AF} & \emph{FULL}      \\
    \hline
    \abltablerow{apache/zookeeper}              {?}            {?}             {?}                               \\
    \abltablerow{apache/hfds}                   {?}            {?}             {?}                               \\
    \abltablerow{apache/hbase}                  {?}            {?}             {?}                               \\
    \hline
    \abltablerow{\textbf{Total}}                {?}            {?}             {?}                               \\
  \end{tabularx}
\end{table}


% a line in tab:case-studies
% arguments: project name, original LoC, # of resources (-AcountMustCall), diff size, # of annotations, TPs, Confirmed TPs, FPs, run time in seconds
\newcommand{\osstablerow}[9]{\textbf{\smaller{#1}} & #2 & #3 & #4 & #5 & #6 & #7 & #8 & #9 s}

\begin{table*}
  \caption{Verifying the absence of resource leaks in case studies.
    Throughout, ``LoC'' is lines of non-comment, non-blank Java code.
    ``Resources'' is the number of resources created by the program.
    ``Diff size'' is the difference in LoC between the original and
    annotated programs, counting both annotations and modified code.
    ``Annos.'' is number of manually-written annotations to specify
    existing methods.
    ``TPs'' is true positives. ``Conf'' is confirmed true positives. 
    ``FPs'' is false positives, where the our analysis could not
  guarantee that the call was safe, but manual analysis revealed that no
  run-time failure was possible.
    ``RT(s)'' is the wall-clock run time of our analysis in seconds.}
  \label{tab:case-studies}

  \begin{tabular}{@{}lrr|rr|rrrr@{}}
    Project                              &      LoC      & Resources   &  Diff size  & Annos.   & TPs  & Conf    & FPs & RT(s)      \\
    \hline
    \osstablerow{apache/zookeeper:zookeeper-server}              {94,872}        {?}            {?}          {99}        {12}     {?}      {44}   {?}        \\
    \osstablerow{apache/hadoop:hdfs}                   {?}        {?}            {?}          {?}        {?}     {?}      {?}   {?}        \\
    \osstablerow{apache/hbase:?}                  {?}        {?}            {?}          {?}        {?}     {?}      {?}   {?}        \\
    \hline
    \osstablerow{\textbf{Total}}                {?}        {?}            {?}          {?}        {?}     {?}      {?}   {-}        \\
  \end{tabular}
\end{table*}


Our evaluation has three parts:
\begin{itemize}
\item case studies on open-source projects, which show that our approach
  scales to realistic programs and finds real bugs (\cref{sec:case-studies}).
\item an ablation study that demonstrates the contributions of
  lightweight ownership system (\cref{sec:lightweight-ownership}),
  resource aliasing (\cref{sec:resource-aliasing}), and
  accumulation frames (\cref{sec:reset-must-call}) to the false positive
  rate on the case studies in \cref{sec:case-studies} (\cref{sec:ablation}).
\item a comparison study that shows the advantages of our approach against
  two traditional approaches: heuristic bug-finders and heavy-weight
  typestate analysis (\cref{sec:compare}).
\end{itemize}

\subsection{Case studies on open-source projects}
\label{sec:case-studies}

We selected \todo{3} popular open-source projects with significant
usage of resources (and thus high danger of resource leaks), by
convenience.
For each project, we selected and analyzed one module
containing significant uses of leakable resources.
We modified the build system of each project to run our
analysis. We manually annotated each program with must-call,
called-methods, and ownership annotations. We also
made small changes to the programs, where possible, to avoid
common false positives of our analysis. We then examined all
remaining warnings, and categorized them as either true
positives---real resource leaks---or false positives---code that our
system was insufficiently precise to prove correct. We submitted bug
reports and patches to the projects describing true positives, when time permitted.
We also measured the number of possible resource leaks in each project
and the run time of our analysis using \todo{a standardized machine similar to the one used
  in the Grapple paper}. Each case study took one author a few hours;
most of the time was spent understanding the code under analysis, which no
authors were familiar with \emph{a priori}, writing corresponding annotations
where appropriate, and reasoning through warnings emitted by the tool by hand
to determine whether a warning was a true or false positive.

\Cref{tab:case-studies} summarizes the results. \tool finds multiple
serious resource leak bugs in all of the examined programs. Though
there are more false positives than true positives in each program,
the number is small enough to be examined by a single developer in a
few hours---a small price to pay for knowing that the program is
definitely free of resource leaks.  The annotations in the program are
also a benefit: as a form of machine-checked documentation, they
express the programmer's intent and, unlike traditional comments,
cannot become out-of-date if the checker is passing.

\todo{Describe some examples of true and false positives.}

\subsection{Ablating lightweight ownership, resource aliasing, and accumulation frames}
\label{sec:ablation}

Without lightweight ownership (\cref{sec:lightweight-ownership}),
resource aliasing (\cref{sec:must-call-choice}), and
accumulation frames (\cref{sec:reset-must-call}), our analysis produces significantly
more false positives. To show the contribution of each to our tool's precision,
we performed an ablation study on the same programs we used as case studies in
\cref{sec:case-studies} by building four versions of our tool:
one with lightweight ownership disabled (\emph{NO-LO}),
one with resource aliaisng disabled (\emph{NO-RA}),
and one with accumulation frames disabled(\emph{NO-AF}),
and the original with all three enabled (\emph{FULL}).
We re-ran the experiments in \cref{sec:case-studies} using each variant, and
recorded the change in the number of warnings. Since our analysis has no false
negatives, the difference between the total number of warnings (i.e. the sum of
the ``TPs'' and ``FPs'' columns in \cref{tab:case-studies}) produced by the \emph{FULL}
variant and the total number of false positives produced by each variant is
the number of false positives that are prevented by each feature.

\Cref{tab:ablation} has an entry for each variant on each case study project,
which is the number of extra warnings that that variant reported on the project,
compared to \emph{FULL}.

\subsection{Comparison to other tools}
\label{sec:compare}

Our approach represents a novel point in the design space of a resource leak checker.

We compared our approach with two other tools that purport to detect resource leaks:
\begin{itemize}
\item an analysis built into the Eclipse Compiler for Java (ecj).
\item Grapple~\cite{}, an unsound, typestate-based analysis that represents a significant, recent
  improvement in the scalability of typestate-based tools that require a whole-program alias analysis.
\end{itemize}

The Grapple paper already evaluated on earlier versions of \todo{some of?} the case study programs in \cref{sec:case-studies}.

We also ran our analysis and ecj's on these versions of the case study programs.

The results presumably show that our tool is faster than grapple and finds more bugs, but has more false positives.
Compared to ecj, we expect to be slower, find more bugs, and have similar numbers of false positives.

\todo{would we present these results as a table?}
