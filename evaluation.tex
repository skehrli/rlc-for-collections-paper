\section{Evaluation}
\label{sec:evaluation}

% TODO: if this table is after tab:case-studies, then it doesn't appear? WTF


% a line in tab:case-studies
% arguments: project name, original LoC, # of resources (-AcountMustCall), diff size, # of annotations, TPs, Confirmed TPs, FPs, run time in seconds
\newcommand{\osstablerow}[8]{\textbf{\smaller{#1}} & #2 & #3 & #4 & #5 & #6 & #7 & #8}

\begin{table*}
  \caption{Verifying the absence of resource leaks in case studies.
    Throughout, ``LoC'' is lines of non-comment, non-blank Java code.
    ``Resources'' is the number of resources created by the program.
    % to compute the diff size, do something like the following (this is what I did for Zookeeper) from the with-annotations branch:
    % > git diff --stat origin/with-checker -- '*.java'
    % then subtract the deleted lines from the added lines and report that number
    ``Diff size'' is the number of lines changed in our annotated version,
    as reported by \<git diff>. Note that this includes warning suppressions
    and annotations.
    ``Annos.'' is number of manually-written annotations to specify
    existing methods.
    ``TPs'' is true positives.
    ``FPs'' is false positives, where the our analysis could not
  guarantee that the call was safe, but manual analysis revealed that no
  run-time failure was possible.
    ``RT(s)'' is the wall-clock run time of our analysis.}
  \label{tab:case-studies}

  \begin{tabular}{@{}lrr|rr|rrr@{}}
    Project                                    &      LoC      & Resources   &  Diff size  & Annos.   & TPs      & FPs & Wall-clock time      \\
    \hline
    \osstablerow{apache/zookeeper:zookeeper-server} {94,872}        {232}       {192}          {99}       {10}       {47}   {?}        \\
    \osstablerow{apache/hadoop:hdfs}                   {?}        {?}            {?}          {?}        {?}        {?}   {?}        \\
    \osstablerow{apache/hbase:?}                       {?}        {?}            {?}          {?}        {?}        {?}   {?}        \\
    \hline
    \osstablerow{\textbf{Total}}                        {?}        {?}            {?}          {?}        {?}       {?}   {-}        \\
  \end{tabular}
\end{table*}


Our evaluation has three parts:
\begin{itemize}
\item case studies on open-source projects, which show that our approach
  scales to realistic programs and finds real bugs (\cref{sec:case-studies}).
\item an ablation study that demonstrates the contributions of
  lightweight ownership system (\cref{sec:lightweight-ownership}),
  resource aliasing (\cref{sec:must-call-choice}), and
  accumulation frames (\cref{sec:reset-must-call}) to the false positive
  rate on the case studies in \cref{sec:case-studies} (\cref{sec:ablation}).
\item a comparison study that shows the advantages of our approach against
  two traditional approaches: heuristic bug-finders and heavy-weight
  typestate analysis (\cref{sec:compare}).
\end{itemize}

\subsection{Case studies on open-source projects}
\label{sec:case-studies}

We selected \todo{3} popular open-source projects with significant
usage of resources (and thus high danger of resource leaks), by
convenience.  For each project, we selected and analyzed one module
containing significant uses of leakable resources.  We modified the
build system of each project to run our analysis. We manually
annotated each program with must-call, called-methods, and ownership
annotations (the ``Annos'' column in \cref{tab:case-studies}). We also
made small, semantics-preserving changes to the programs where
possible---such as adding \<final> modifiers to fields---to avoid
false positives from our analysis. We report the total number
of new lines we added to each project, including warning suppressions,
annotations, and code changes (``Diff size''). We then examined
all remaining warnings, and categorized them as either true positives
(``TPs'')---real resource leaks---or false positives (``FPs'')---code
that our system was insufficiently precise to prove correct.
% We should probably remove this, since we've only done what, one?
We also measured the number of
possible resource leaks in each project (``resources'') and the run
time of our analysis (``Wall-clock time'') using \todo{a standardized
  machine similar to the one used in the Grapple paper}. Each case
study took one author a few hours; most of the time was spent
understanding the code under analysis, which no authors were familiar
with before beginning; writing corresponding annotations where
appropriate; and reasoning through warnings emitted by the tool by
hand to determine whether a warning was a true or false positive.  The
annotated versions of the case study programs are available at
\todo{an anonymized fork of the projects}.

\Cref{tab:case-studies} summarizes the results. \Tool finds multiple
serious resource leak bugs in all of the examined programs. Though
there are more false positives than true positives in each program,
the number is small enough to be examined by a single developer in a
few hours---a small price to pay for knowing that the program is
definitely free of resource leaks.  The annotations in the program are
also a benefit: as a form of machine-checked documentation, they
express the programmer's intent and, unlike traditional comments,
cannot become out-of-date if the checker is passing.

\todo{Describe some examples of true and false positives.}
\todo{Definitely discuss our confirmed bug that Narges got fixed.}

\todo{Discuss the annotations that had to be written by hand.}

\subsection{Ablating lightweight ownership, resource aliasing, and accumulation frames}
\label{sec:ablation}

% a line in tab:ablation
% note that the FULL column is always zero, so it's not included here
% arguments: project name, no-LO, no-RA, no-AF
\newcommand{\abltablerow}[4]{\textbf{\smaller{#1}} & #2 & #3 & #4}

\begin{table}
  \caption{An ablation study on the contribution of the lightweight
    ownership (\emph{without-LO}), resource aliasing (\emph{without-RA}),
    and accumulation frames (\emph{without-AF})
    features in reducing false positives. Each entry is the number of extra
    false positive warnings reported by the variant with the given feature disabled on the given project.}
  \label{tab:ablation}
  
  \begin{tabularx}{\columnwidth}{@{}Xrrr@{}}
    Project                              &      \emph{without-LO} & \emph{without-RA} & \emph{without-AF}     \\
    \hline
    \abltablerow{apache/zookeeper}              {66}            {97}             {10}                               \\
    \abltablerow{apache/hadoop}                   {?}            {?}             {?}                               \\
    \abltablerow{apache/hbase}                  {?}            {?}             {?}                               \\
    \hline
    \abltablerow{\textbf{Total}}                {?}            {?}             {?}                               \\
  \end{tabularx}
\end{table}

Lightweight ownership (\cref{sec:lightweight-ownership}),
resource aliasing (\cref{sec:must-call-choice}), and
accumulation frames (\cref{sec:reset-must-call})
each contribute to \Tool's precision by eliminating false positives.
To show the contribution of each,
we performed an ablation study on the same programs we used as case studies in
\cref{sec:case-studies} by building three variants of \Tool:
one with lightweight ownership disabled (\emph{without-LO}),
one with resource aliaisng disabled (\emph{without-RA}),
and one with accumulation frames disabled(\emph{without-AF}).
We re-ran the experiments in \cref{sec:case-studies} using each variant, and
recorded the change in the number of warnings. Since our analysis has no false
negatives, the difference between the total number of warnings (i.e. the sum of
the ``TPs'' and ``FPs'' columns in \cref{tab:case-studies}) produced by \Tool
and the total number of false positives produced by each variant is
the number of false positives that are prevented by each feature.

\Cref{tab:ablation} has an entry for each variant on each case study project,
which is the number of extra warnings that that variant reported on the project.
From the table, it is clear that both lightweight
ownership and resource aliases are critically important to the precision of
\Tool: each feature prevents more false positive errors than the total number
of remaining false positives on the Zookeeper benchmark, for example. \todo{Strengthen
  this claim when we have the results from the other benchmarks.} Though
the accumulation frame feature prevents fewer errors, it permits verification
of a complex pattern---the use of non-final, owning fields---that is used in a
few critical places in each benchmark.

\subsection{Comparison to other tools}
\label{sec:compare}

Our approach represents a novel point in the design space of a resource leak checker.

In this section, we compare our approach with two other modern tools that purport to detect resource leaks:
\begin{itemize}
\item the analysis built into the Eclipse Compiler for Java (ecj), which is the default approach
  for detecting resource leaks in the Eclipse IDE~\cite{ecj-resource-leak}.
\item Grapple~\cite{zuo2019grapple}, an unsound, typestate-based analysis that represents a significant, recent
  improvement in the scalability of typestate-based tools that require a whole-program alias analysis.
\end{itemize}
Each tool is evaluated against the four criteria that we proposed in \cref{sec:intro}
for an ideal resource-leak detector: analysis speed, applicability to legacy code, soundness (i.e. ability
to detect all errors), and precision (i.e. lack of false positive warnings).

\subsubsection{Eclipse}
\label{sec:eclipse}

The Eclipse analysis is a simple dataflow analysis
augmented with heuristics. Because it is tightly integrated with
the compiler, it scales well and runs quickly. It contains
heuristics for ownership, resource wrappers, and resource-free
closeables, among others; these are all hard-coded into the analysis and cannot
be adjusted by the user. It does not support annotations to express
specifications in these categories that differ from its defaults.
It supports two levels of analysis: detecting ``potential'' resource
leaks and detecting high-confidence resource leaks; the former reports a superset
of the warnings reported by the latter.

We ran this analysis on the same version of Zookeeper's zookeeper-server
module that we ran \Tool on in \cref{sec:case-studies}. Once Eclipse
has loaded the project, the analysis runs nearly instantaneously---it
clearly satisfies the ``fast'' criterion. It is also easy to apply it to
existing legacy code---it clearly satisfies the ``applicable'' criterion.

In potential leak mode on zookeeper-server, the analysis reported 180
warnings.  Of these, we determined that 7 were true positives
(corresponding to true positives that our sound tool found), and the
other 173 were false positives.  The most common cause of false
positives was the unchangeable, default ownership transfer assumption
at method invocations: each call to a method such as
\<Socket\#getInputStream> that returns a resource-alias resulted in a
warning.  Of the true positives, 4 of 7 were warnings about such
calls, where the aliased resource really was not closed on all
paths. We believe that these true positives are ``accidental''---the
analysis found them because of its imprecise handling of resource
aliases, not because it detected the actual leak. When the
``potential'' resource leaks are ignored in ``high-confidence'' mode,
Eclipse reports only 3 warnings: one true positive and two false
positives.

These results demonstrate that the Eclipse bug-finder is unsound in both
modes: it misses at least 3 real warnings that our tool found when detecting
potential leaks. Further, the analysis is imprecise: in potential leak mode,
the vast majority of warnings are false positives. However, this sort
of bug-finding tool does have two advantages: it is fast, and it is easily
applicable to legacy code. Compared to our tool, the Eclipse analysis
is much faster and equally applicable to legacy code, but both less sound
and less precise.

%% Chandra rightly pointed out that this para make us look lazy. Let's just
%% say what we did, not why we did it.
%%
%% Given that these results on zookeeper-server
%% clearly show the strengths and weaknesses of this tool compared to our own,
%% we did not run it on our other case study programs.

\subsubsection{Grapple}
\label{sec:grapple}

% a line in tab:grapple
% these numbers come from tables 2 and 3 in the grapple paper from EuroSys.
% for TPs and FPs, I added together the IO and Socket columns of table 2.
\newcommand{\grappletablerow}[4]{\textbf{\smaller{#1}} & #2 & #3 & #4}

\begin{table}
  \caption{Key findings on the Grapple tool's performance on the case study
    programs in \cref{sec:case-studies}; reproduced from~\cite{zuo2019grapple}.}
  \label{tab:grapple}
  
  \begin{tabularx}{\columnwidth}{@{}Xrrr@{}}
    Project                              &  TPs    &    FPs         & Run time      \\
    \hline
    \grappletablerow{ZooKeeper}             {6}         {0}           {01h07m02s}     \\
    \grappletablerow{HDFS}                  {5}         {2}           {01h54m52s}    \\
    \grappletablerow{HBase}                 {15}        {2}           {33h51m59s}     \\
    \hline
    \grappletablerow{\textbf{Total}}        {26}        {4}           {-}          \\
  \end{tabularx}
\end{table}

\todo{Be very sure that Grapple is unsound - read their paper again, and
   point out why in this paragraph.}
\todo{be sure to note that Grapple is not modular, unlike us}
Grapple~\cite{zuo2019grapple} is a modern typestate-based resource leak analysis
focused on precision and (relative) scalability. Grapple models its alias and
dataflow analyses as dynamic transitive-closure computation of graph
representations of the target programs. Grapple, like all previous
typestate-based analyses, requires a whole-program alias analysis. Grapple
contains four checkers, of which two (IO and Socket) are useful for detecting
resource leaks.

The Grapple authors have already evaluated their tool on earlier
versions of the case study programs in
\cref{sec:case-studies}~\cite{zuo2019grapple}.  Unfortunately, we were
not able to run the tool on the same versions of the case study
programs that we used in \cref{sec:case-studies}.
\todo{Manu: explain why we couldn't/didn't try to do this.}
We reproduce their key findings below in \cref{tab:grapple}
(their run time numbers were computed on a commodity machine
\todo{similar to} the commodity machine we used to compute the run
times that appear in \cref{tab:case-studies}, but includes the cost
of running all four of their checkers).

Compared to our approach, Grapple is slower (due to its reliance on
an expensive whole-program alias analysis, which \Tool avoids by using
an accumulation analysis), similarly applicable to legacy
code, less sound, and much more precise--our type-based approach
trades off precision and a small amount of user effort (i.e. to write annotations)
for soundness and speed. In this way, \Tool and Grapple are complementary:
they optimize for different criteria. Users should select the tool whose
priorities match their own.

\todo{We should note here (or somewhere) the difference in the nature of the bugs
reported between our approach and whole-program analysis, in particular for
class-level issues.  E.g., if there is a class with a method \<m()> that
overwrites a non-final \<@Owning> field \<f> without checking its nullness
first, we consider that a true positive bug, since the abstraction is not
provably free of leaks.  In a whole-program analysis approach, a true positive
would typically have to be an actual control-flow path on which \<m()> is
invoked twice to overwrite \<f>.  (Maybe this can be discussed under
modularity?)  Bottom line, it makes TP and FP numbers between the approaches not
100\% comparable.  --Manu}

