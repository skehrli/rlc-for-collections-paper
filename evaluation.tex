\section{Evaluation}
\label{sec:evaluation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% tab:case-studies is in accumulation-frames.tex so that it
%% appears on the first page of the eval.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our evaluation has three parts:
\begin{itemize}
\item case studies on open-source projects, which show that our approach
  is scalable and finds real bugs (\cref{sec:case-studies}).
\item an evaluation of the contributions of
  lightweight ownership, % (\cref{sec:lightweight-ownership}),
  resource aliasing, % (\cref{sec:must-call-choice}), and
  and obligation creation % (\cref{sec:reset-must-call})
  (\cref{sec:ablation}).
\item a comparison to previous leak detectors:  both heuristic bug-finding
  and heavy-weight whole-program
  % typestate
  analysis (\cref{sec:compare}).
\end{itemize}
\noindent
\paragraph{Results} All code and data for the experiments described
in this section, including \tool's implementation, our experimental
machinery, and the annotated versions of our case study programs,
are publicly available at \todo{permanent artifact URL}.

\subsection{Case studies on open-source projects}
\label{sec:case-studies}

We selected 3 popular open-source projects that were analyzed by prior work~\cite{zuo2019grapple}.
For each, we selected and analyzed one or two modules
with many uses of leakable resources. We used
the latest version of the source code that was available
when we began. We also analyzed a smaller open-source project
maintained by one of the authors, to better simulate \tool's
expected use case, where the user is already familiar with the
code under analysis (see \cref{sec:plume-util}).

Our methodology was:
(1)
We modified the
build system to run our analysis on the module, analyzing uses of resource
classes that are defined in the JDK\@.
It also reports the maximum possible number of resources that could be
leaked:  each obligation at a formal parameter or method call.
(2) We manually
annotated each program with must-call, called-methods, and ownership
annotations (see \cref{sec:annos}).
% the ``Annos'' column in \cref{tab:case-studies}).
(3) We iteratively ran the analysis to correct our annotations.
We measured the run time
as the median of 5 trials on
% This is Mike's home machine.  Are there any other relevant parameters?
a machine with an Intel Core i7-10700 CPU running at 2.90GHz and 64GiB of RAM\@.
Our analysis is embarassingly parallel, but our implementation is
single-threaded because javac is single-threaded.
(4) We manually categorized each warning as revealing a
real resource leak (a true positive or TP) or as correct code that our
system is unable to prove correct (a false positive or FP\@).

\Cref{tab:case-studies} summarizes the results. \Tool found multiple
serious resource leak bugs in every program. \Tool issues
more false positives than true positives in each program, but
the number is small enough to be examined by a single developer in a
few hours.  This is a small price to pay for knowing that the program is
free of resource leaks.  The annotations in the program are
also a benefit: as a form of machine-checked documentation, they
express the programmer's intent and, unlike traditional comments,
cannot become out-of-date if the checker is passing.

%% Here's the full table of annotations:

%% -              ZK  HB  HD PU  Total
%% Owning         34  11  29 0   74
%% NotOwning      8   5   11 0   24  = 98
%% ----
%% EnsuresCM      25  11  18 0   54  = 54
%% ---
%% MustCall       13  7   17 0   37
%% InheritableMC  5   4   7  0   16  = 53
%% ---
%% MustCallAlias  4   2   28 2   36
%% PolyMustCall   4   0   0  0   4   = 40
%% ---
%% CO             29  5   6  0   40  = 40


\begin{table}
  \caption{The total number of annotations that we wrote.}
  \label{tab:annos}
  \posttablecaption
  % counts @InheritableMustCall as @MustCall, for simplicity of presentation
  % counts @polymustcall with MCC, for the same reason
  \begin{tabularx}{\columnwidth}{@{}Xr@{}}
    Annotation                           &      Count     \\
    \hline
    \<@Owning> and \<@NotOwning>            &      98   \\
    \<@EnsuresCalledMethods>                &      54       \\
    \<@MustCall>                            &      53       \\
    \<@MustCallAlias>                       &      40       \\
    \CreatesObligation                       &     40      \\
  \end{tabularx}
\end{table}

\subsubsection{True and false positive examples}
\label{sec:examples}

This section describes some examples of warnings reported by \tool
in our case studies.\looseness=-1

\begin{figure}
  \lstinputlisting{hadoop-bug.txt}
  \prefigcaption
  \caption{A true positive that \tool found in Hadoop. Our pull request
    to fix this bug was accepted by Hadoop's developers.}
  \label{fig:hadoop-bug}
\end{figure}

\Cref{fig:hadoop-bug} contains code from Hadoop. If an IO error
occurs any time between the allocation of the \<FileInputStream>
in the first line of the method and the \<return> statement
at the end---for example, if \<channel.position(section.getOffset())>
throws an \<IOException>, as it is specified to do---then the
only reference to the stream is lost. Hadoop's developers
assigned this bug a priority of ``Major'' and accepted our
patch.\footnoteanonurl{https://github.com/apache/hadoop/pull/2652}
One developer suggested using a try-with-resources statement instead
of our patch (which catches the exception and closes the stream),
but we pointed out that
the file needs to remain open if no error occurs so that it can be
returned.  \manu{this example is also interesting in that verifying the caller
is non-trivial.  dunno if we have space, but if we do, we may want to include}
\todo{say something like we are in the process of submitting more bug reports and fixes?}  

\begin{figure}
  \lstinputlisting{zookeeper-optional.txt}
  \prefigcaption
  \caption{Code from the ZooKeeper case study that causes \tool
  to issue a false positive.}
  \label{fig:zookeeper-optional}
\end{figure}

The most common false
positive pattern (with \todo{15} instances) was caused by
a generic container object like \<java.util.Optional> taking ownership of a resource, such
as the example in \cref{fig:zookeeper-optional}. Our lightweight ownership
system does not support transferring ownership to generic parameters,
so \tool issues an error when \<Optional.of> is returned. In this case, the use
of the \<Optional> class complicates the code; if \<Optional> was replaced
by \<null>,
%% Leave out of submitted version.
% as some advocate~\cite{ErnstNothingIsBetterThanOptional},
\Tool could verify this code. We leave expanding the lightweight ownership system to
support Java generics as future work.

Another common false positive pattern (with \todo{?} instances)
was caused by the Checker Frameworkâ€™s overly-conservative
type inference algorithm for Java generics~\cite{issue979}.

Most false positives involve unique coding patterns.
One example is a series of catch
statements that each sets an \<error> boolean to \<true>, and
a \<finally> statement that closes the relevant socket if \<error>
was true.  \Tool reports an error because it does not reason about path
conditions, and handling such reasoning is future work.\looseness=-1

\subsubsection{Annotations and code changes}
\label{sec:annos}

%%%%%%%%%%%%%%%%
%% This is where tab:annos belongs, but I moved it earlier in this file
%% to make the figures line up nicely.
%%%%%%%%%%%%%%%%

% Because most relevant \MustCall and \<@CalledMethods> annotations
% are inferred intra-procedurally, most annotations that we had to write in
% the case studies pertained to ownership or the side-effects of procedures.
We wrote about one annotation per 1,500 lines of code (\cref{tab:annos}).

We also
made 38 small, semantics-preserving changes to the programs to reduce
false positives from our analysis.
%
In 19 places in plume-util, we added an explict \<extends> bound to a generic type.
The Checker Framework uses different defaulting rules for implicit and explicit
upper bounds, and a common pattern in this benchmark caused our checker to issue
an error on uses of implicit bounds.
%
In 8 places, we added \<final> to a field; this allows our checker to verify it without using
the rules for non-final owning fields given in \cref{sec:reset-must-call}, which are stricter.
In 3 of those, we also removed assignments to the field after it was closed whose right-hand
side was \<null>; in 1 other we added an \<else> clause in the constructor that assigned the field
a \<null> value.
%
In 3 places, we re-ordered two statements to remove an
infeasible control-flow-graph edge.
% In 2 places, we re-ordered a \<try> statement and a null-check.
% % In 1 place, we re-ordered two calls to \<close()> that closed
% the same resource (via resource aliases). The first call in the
% unmodified code was conditional, but the second was
% not, leading to an infeasible control-flow graph edge.
%
In 2 places, we extracted an expression into a local variable, permitting
flow-sensitive reasoning or targetting by a \CreatesObligation annotation.
% multiple calls to a method
% into a single call that is assigned to a local variable; this allows
% flow-sensitive reasoning about the local
% variable.
% % , and the code is also more efficient.
% In 1 place, we extracted an expression into a local variable
% so that a \CreatesObligation annotation could target it.

\subsubsection{Simulating the user experience}
\label{sec:plume-util}

To better simulate the user experience of a typical user,
one author used \tool to analyze plume-util,
a library (written years ago for a different project).
The process only took about two hours (including running the tool,
writing annotations, and fixing the bugs that were discovered by the tool),
largely because the author already understood
both \tool and the program.  The annotations were valuable enough that they
are now committed to that codebase, and \tool runs in CI on every commit
to prevent the introduction of new resource leaks.
This example is \emph{not} a proof, but it is suggestive of
our tool's generality and that the annotation burden is reasonable---when
the user is familiar with both \tool and the code being analyzed, writing
annotations is fast (even when real bugs are discovered and must be fixed).

\subsection{Evaluating our enhancements}
\label{sec:ablation}

% a line in tab:ablation
% note that the FULL column is always zero, so it's not included here
% arguments: project name, no-LO, no-RA, no-AF
\newcommand{\abltablerow}[4]{\textbf{\smaller{#1}} & #2 & #3 & #4}

\begin{table}
  \caption{The contribution of the lightweight
    ownership, resource aliasing,
    and obligation creation features
    in reducing false positives. Each entry is the number of extra
    false positive warnings reported by the variant with the given feature disabled on the given project.}
  \label{tab:ablation}
  \posttablecaption
  
  \begin{tabularx}{\columnwidth}{@{}Xrrr@{}}
    Project                              &      without LO & without RA & without CO     \\
    \hline
    \abltablerow{apache/zookeeper}              {66}            {97}             {10}                               \\
    \abltablerow{apache/hadoop}                   {48}            {135}             {3}                               \\
    \abltablerow{apache/hbase}                  {60}            {71}             {4}                               \\
    \hline
    \abltablerow{\textbf{Total}}                {174}            {303}             {17}                               \\
  \end{tabularx}
\end{table}

The base analysis of \cref{sec:base-type-systems} produces significantly
more false positives, because it lacks 
lightweight ownership (\cref{sec:lightweight-ownership}),
resource aliasing (\cref{sec:must-call-choice}), and
obligation creation (\cref{sec:reset-must-call}).
Each contributes to \tool's precision by eliminating false positives.
To evaluate the contribution of each enhancement, we individually disabled each
feature and re-ran the experiments of \cref{sec:case-studies}
(except the plume-util case study, which is small and
barely uses these features, so the results would not be meaningful).
Since all variants are sound (no false
negatives), any difference in warnings is a false positive that is prevented
by the feature.

\Cref{tab:ablation} shows that each of lightweight
ownership and resource aliases prevents more false positive errors than the total number
of remaining false positives on each benchmarks---showing that removing either
would make our technique produce an unreasonable number of false positives.
The system for creating new obligations at points other than constructors reduces
false positives by a smaller amount: non-final, owning field re-assignments are rare;
and we encountered the unconnected socket pattern described in \cref{sec:unconnected-sockets}
only in ZooKeeper. Nevertheless, this feature allows our tool to handle an
important, if less common, coding pattern.

\subsection{Comparison to other tools}
\label{sec:compare}

Our approach represents a novel point in the design space of resource leak checkers.
%
This section compares our approach with two other modern tools that detect resource leaks:
\begin{itemize}
\item The analysis built into the Eclipse Compiler for Java (ecj), which is the default approach
  for detecting resource leaks in the Eclipse IDE~\cite{ecj-resource-leak}.
  We used version 4.18.0.
\item Grapple~\cite{zuo2019grapple} represents a significant, recent
  improvement in the scalability of typestate-based tools that require a whole-program alias analysis.
\end{itemize}
In brief, both tools are unsound.
Both tools are applicable to legacy code. % , without the need to write annotations.
Eclipse is very fast (nearly instantaneous) and suffers 67--98\% false
positives. % while missing 70-90\% of resource leaks.
According to its authors, Grapple is precise (13\% false positive rate) but orders
of magnitude slower than \tool.
Different users can select whichever tool matches
their priorities.


\subsubsection{Eclipse}
\label{sec:eclipse}

The Eclipse analysis is a simple dataflow analysis
augmented with heuristics. Since it is tightly integrated with
the compiler, it scales well and runs quickly. It has
heuristics for ownership, resource wrappers, and resource-free
closeables, among others; these are all hard-coded into the analysis and cannot
be adjusted by the user. It does not support annotations to express
specifications that differ from its defaults.
It supports two levels of analysis: detecting high-confidence resource
leaks and detecting ``potential'' resource
leaks (a superset of high-confidence resource leaks).\looseness=-1

We ran Eclipse's analysis on the same versions of our case study programs
that we ran \tool on in \cref{sec:case-studies}, and examined
the same modules. It is easy to apply
to legacy code and it is fast---nearly instantaneous once Eclipse
has loaded the project.\looseness=-1

In ``high-confidence'' mode on the three projects, Eclipse reports 9
warnings related to classes defined in the JDK:
2 true positives (thus, it misses 43 real resource leaks) and 7
false positives.
In ``potential'' leak mode, the analysis reports many more warnings.
Thus, we triaged only the 180
warnings about JDK classes from the ZooKeeper benchmark.
Among these were 3 true positives (it misses 9 real resource leaks) and 177 false
positives .
The most common cause of false
positives was the unchangeable, default ownership transfer assumption
at method invocations:
the potential leak mode warned at each call that returns a resource-alias, such as
\<Socket\#getInputStream>.

These results show that the Eclipse bug-finder is unsound in both
modes: it misses 9 real warnings on ZooKeeper even in potential leak mode.
The analysis is imprecise, too: in potential leak mode,
the vast majority of warnings are false positives, and even in high-confidence
mode its ratio of true to total warnings is higher than \tool's: (2/9 = 22\%
vs. 45/156 = 29\%).
%% MK: I think this sentence is redundant
%% However, this sort
%% of bug-finding tool does have two advantages: it is fast, and it is easily
%% applicable to legacy code.
Compared to our tool, the Eclipse analysis
is much faster and easier to apply to legacy code
(no annotations are required), but both less sound
and less precise.\looseness=-1

%% Chandra rightly pointed out that this para make us look lazy. Let's just
%% say what we did, not why we did it.
%%
%% Given that these results on zookeeper-server
%% clearly show the strengths and weaknesses of this tool compared to our own,
%% we did not run it on our other case study programs.

\subsubsection{Grapple}
\label{sec:grapple}

% a line in tab:grapple
% these numbers come from tables 2 and 3 in the grapple paper from EuroSys '19.
% for TPs and FPs, I added together the IO and Socket columns of table 2.
\newcommand{\grappletablerow}[4]{\textbf{\smaller{#1}} & #2 & #3 & #4}

\begin{table}
  \caption{The Grapple tool's performance; reproduced from~\cite{zuo2019grapple}.
  \todo{Clarify relationship of ``HDFS'' in this table to our corresponding benchmark.}}
  \label{tab:grapple}
  \posttablecaption
  
  \begin{tabularx}{\columnwidth}{@{}Xrrr@{}}
    Project                              &  TPs    &    FPs         & Run time      \\
    \hline
    \grappletablerow{ZooKeeper}             {6}         {0}           {01h 07m 02s}     \\
    \grappletablerow{HDFS}                  {5}         {2}           {01h 54m 52s}    \\
    \grappletablerow{HBase}                 {15}        {2}           {33h 51m 59s}     \\
    \hline
    \grappletablerow{\textbf{Total}}        {26}        {4}           {-}          \\
  \end{tabularx}
\end{table}

% \todo{Be very sure that Grapple is unsound - read their paper again, and
%    point out why in this paragraph.}
%\todo{be sure to note that Grapple is not modular, unlike us}
Grapple~\cite{zuo2019grapple} is a modern typestate-based resource leak analysis
focused on high precision and (relative) scalability. Grapple models its alias and
dataflow analyses as dynamic transitive-closure computations over graphs, and
leverages novel path encodings and techniques from predecessor-system
Graspan~\cite{wang2017graspan} to achieve both context- and path-sensitivity.  
% Grapple, like all previous
% typestate-based analyses, requires a whole-program alias analysis.
Grapple contains four checkers, of which two are useful for detecting
resource leaks.  Unlike \tool, Grapple is unsound; e.g., it performs a fixed bounded unrolling
of loops to make path sensitivity tractable.

The Grapple authors have already evaluated their tool on earlier
versions\todo{Why don't we repeat our experiments on those earlier versions?
This should be easy to do since it's only a few hours of work and we are now
familiar with the programs.} of the case study programs in
\cref{sec:case-studies}~\cite{zuo2019grapple}; \Cref{tab:grapple} reproduces
results from their paper.  Based on these numbers, Grapple reports fewer false
positives than \tool.  Unfortunately, we could not run
Grapple's leak detection on our versions of the benchmarks, due to various
hardcoded paths in its source code and limited documentation. Also, full details
on Grapple's true and false-positive warnings are currently unavailable (we have
requested them from the authors), so we cannot study the precision differences
further.  The run times in \Cref{tab:grapple} show a stark difference with our
work; \tool runs in minutes, whereas Grapple can take many hours. Further, to
our best knowledge, Grapple is not modular, so any code modification will
necessitate a full re-analysis. \Tool only needs to re-analyze modified code and
possibly its dependents after a change, not unmodified dependencies.\looseness=-1

% We requested the analysis and Grapple's output, but as of this writing
% (months later) the Grapple authors have not yet provided them.
% \manu{I think this is strictly-speaking true but a bit harsh.  We
% may want to tone it down a bit.}

\todo{Still keep this?} Note that the TP and FP numbers are not perfectly comparable between a modular
and a whole-program analysis.
Our tool reports violations of a user-supplied specification
(which takes effort to write but provides documentation benefits), so it
can ensure that a library is correct for all possible clients.  By
contrast, Grapple checks a library in the context of one specific client.

% LocalWords:  LoC SHAs leakable
