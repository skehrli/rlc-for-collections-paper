\section{Evaluation}
\label{sec:evaluation}

% Move this table earlier in the LaTeX source if needed; at present, this
% location puts it as early as possible without being on a page preceding
% the first reference to it.
\input{case-studies-table.tex}

Our evaluation has three parts:
\begin{itemize}
\item case studies on open-source projects, which show that our approach
  is scalable and finds real resource leaks (\cref{sec:case-studies}).
\item an evaluation of the importance of
  lightweight ownership, % (\cref{sec:lightweight-ownership}),
  resource aliasing, % (\cref{sec:must-call-choice}), and
  and obligation creation % (\cref{sec:reset-must-call})
  (\cref{sec:ablation}).
\item a comparison to previous leak detectors:  both a heuristic bug finder
  and a whole-program
  % typestate
  analysis (\cref{sec:compare}).
\end{itemize}

% \noindent
% \paragraph{Results}

All code and data for our experiments described
in this section, including \tool's implementation, our experimental
machinery, and the annotated versions of our case study programs,
are publicly available at \todo{permanent artifact URL}.

\subsection{Case studies on open-source projects}
\label{sec:case-studies}

We selected 3 popular open-source projects that were analyzed by prior work~\cite{zuo2019grapple}.
For each, we selected and analyzed one or two modules
with many uses of leakable resources. We used
the latest version of the source code that was available
when we began. We also analyzed a smaller open-source project
maintained by one of the authors, to better simulate \tool's
expected use case, where the user is already familiar with the
code under analysis (see \cref{sec:plume-util}).

For each case study, our methodology was as follows.
(1)
We modified the
build system to run \tool on the module(s), analyzing uses of resource
classes that are defined in the JDK\@.
It also reports the maximum possible number of resources (references to JDK-defined
classes with a non-empty \<@MustCall> obligation) that could be
leaked:  each obligation at a formal parameter or method call.
(2) We manually
annotated each program with must-call, called-methods, and ownership
annotations (see \cref{sec:annos}).
% the ``Annos'' column in \cref{tab:case-studies}).
(3) We iteratively ran the analysis to correct our annotations.
We measured the run time
as the median of 5 trials on
% This is Mike's home machine.  Are there any other relevant parameters?
a machine running Ubuntu 20.04 with an Intel Core i7-10700 CPU running at 2.90GHz and 64GiB of RAM\@.
Our analysis is embarrassingly parallel, but our implementation is
single-threaded because javac is single-threaded.
(4) We manually categorized each warning as revealing a
real resource leak (a true positive) or as a
false positive that our system is unable to prove correct.

\Cref{tab:case-studies} summarizes the results. \Tool found multiple
serious resource leaks in every program. \Tool issues
more false positives than true positives in each program\todo{Rather than
the preceding sentence, just report overall precision as a concrete number.}
\todo{update sentence to account for plume-util?  take average across all benchmarks?}, but
the number is small enough to be examined by a single developer in a
few hours.  This is a small price to pay for knowing that the program is
free of resource leaks.  \todo{technically we don't know that since we only
investigated warnings from JDK types; delete previous sentence?} The annotations in the program are
also a benefit: they
express the programmer's intent and, as machine-checked documentation,
they cannot become out-of-date.

\todo{does this para need to be updated for plume-util?}
At the time of writing, we have submitted patches for 16 resource leaks
discovered by our tool, all of which have been validated and accepted by the
case study program developers.  \todo{do we need the next sentence?} No patches
we have submitted this way have been rejected.  We also fixed all the reported
resource leaks in plume-util.  \todo{Should we also include the links to the patches here?
I'm not sure how to do that in a way that's not overly intrusive...I guess we
could add a citation for each one?}\todo{Mike suggests that we put that
information in the supplemental material/artifact.  The text here could
say they are there.}

\subsubsection{True and false positive examples}
\label{sec:examples}

This section describes some examples of warnings reported by \tool
in our case studies.\looseness=-1

\begin{figure}
  \lstinputlisting{hadoop-bug.txt}
  \prefigcaption
  \caption{A resource leak that \tool found in Hadoop. Our pull request
    to fix this it was accepted by Hadoop's developers.}
  \label{fig:hadoop-bug}
\end{figure}

\Cref{fig:hadoop-bug} contains code from Hadoop. If an IO error
occurs any time between the allocation of the \<FileInputStream>
in the first line of the method and the \<return> statement
at the end---for example, if \<channel.position(section.getOffset())>
throws an \<IOException>, as it is specified to do---then the
only reference to the stream is lost. Hadoop's developers
assigned this issue a priority of ``Major'' and accepted our
patch.\footnoteanonurl{https://github.com/apache/hadoop/pull/2652}
One developer suggested using a try-with-resources statement instead
of our patch (which catches the exception and closes the stream),
but we pointed out that
the file needs to remain open if no error occurs so that it can be
returned.

\begin{figure}
  \lstinputlisting{zookeeper-optional.txt}
  \prefigcaption
  \caption{Code from the ZooKeeper case study that causes \tool
  to issue a false positive.}
  \label{fig:zookeeper-optional}
\end{figure}

The most common reason for false
positives (with 24 instances\todo{Readers don't care about the
  number of instances, only the frequency.  Replace this absolute number by
  a percentage.}) was
a known bug in the Checker Frameworkâ€™s
type inference algorithm for Java generics~\cite{issue979}.
The second most common reason (with 15 instances\todo{Replace this
  absolute number by a percentage.})
was a generic container object like \<java.util.Optional> taking ownership of a resource, such
as the example in \cref{fig:zookeeper-optional}. Our lightweight ownership
system does not support transferring ownership to generic parameters,
so \tool issues an error when \<Optional.of> is returned. In this case, the use
of the \<Optional> class is unnecessary and complicates the
code~\cite{ErnstNothingIsBetterThanOptional}.  If \<Optional> was replaced
by a nullable Java reference,
\tool could verify this code. Future work should expand the lightweight ownership system to
support Java generics.

Most false positives involve unique coding patterns.
One example\todo{It's weird to give a random reason.  Replace this by the
  third most common pattern, and state its prevalence as a percentage.
  Readers will see the percentages and know
  that all the other reasons are in the noise.}
 is a series of catch
statements that each sets an \<error> boolean to \<true>, and
a \<finally> statement that closes the relevant socket if \<error>
was true.  \Tool reports an error because it does not reason about path
conditions.

\subsubsection{Annotations and code changes}\label{sec:annotations-and-code-changes}
\label{sec:annos}

\input{table-annotations.tex}

% Because most relevant \MustCall and \<@CalledMethods> annotations
% are inferred intra-procedurally, most annotations that we had to write in
% the case studies pertained to ownership or the side-effects of procedures.
We wrote about one annotation per 1,500 lines of code (\cref{tab:annos}).

We also
made 42 small, semantics-preserving changes to the programs to reduce
false positives from our analysis.
%
In 19 places in plume-util, we added an explicit \<extends> bound to a generic type.
The Checker Framework uses different defaulting rules for implicit and explicit
upper bounds, and a common pattern in this benchmark caused our checker to issue
an error on uses of implicit bounds.
%
In 18 places, we made a field \<final>; this allows our checker to verify usage
of the field without using the stricter rules for non-final owning fields given in \cref{sec:reset-must-call}.
In 9 of those cases, we also removed assignments of \<null> to the field after it was closed; in 1 other we added an \<else> clause in the constructor that assigned the field
a \<null> value.
%
In 3 places, we re-ordered two statements to remove an
infeasible control-flow-graph edge.
% In 2 places, we re-ordered a \<try> statement and a null-check.
% % In 1 place, we re-ordered two calls to \<close()> that closed
% the same resource (via resource aliases). The first call in the
% unmodified code was conditional, but the second was
% not, leading to an infeasible control-flow graph edge.
%
In 2 places, we extracted an expression into a local variable, permitting
flow-sensitive reasoning or targeting by a \CreatesMustCallFor annotation.
% multiple calls to a method
% into a single call that is assigned to a local variable; this allows
% flow-sensitive reasoning about the local
% variable.
% % , and the code is also more efficient.
% In 1 place, we extracted an expression into a local variable
% so that a \CreatesMustCallFor annotation could target it.

\subsubsection{Simulating the user experience}
\label{sec:plume-util}

To better simulate the user experience of a typical user who understands
the codebase being analyzed,
one author used \tool to analyze plume-util,
a library he wrote 24 years ago.
The process only took about two hours, including running the tool,
writing annotations, and fixing the 8 resource leaks that were discovered by the tool.
The annotations were valuable enough that they
are now committed to that codebase, and \tool runs in CI
to prevent the introduction of new resource leaks.
This example is suggestive that the user effort to use our tool is reasonable.


\subsection{Evaluating our enhancements}
\label{sec:ablation}

% a line in tab:ablation
% arguments: project name, no-LO + total, no-RA + total, no-AF + total, and total FP count
\newcommand{\abltablerow}[5]{\textbf{\smaller{#1}} & #2 & #3 & #4 & #5}

\begin{table}
  \caption{The number of false positives in our case studies when each
    of the lightweight
    ownership (``w/o LO''), resource aliasing (``w/o RA''),
    and obligation creation (``w/o OC'') features
    are disabled. For comparison, the ``RLC'' column lists the number of false
    positives with all features enabled.}
  \label{tab:ablation}
  \posttablecaption
  
  \begin{tabularx}{\columnwidth}{@{}Xrrrr@{}}
    Project                              &    w/o LO & w/o RA & w/o OC & RLC     \\
    \hline
    \abltablerow{apache/zookeeper}              {117}            {158}             {54}      {48}                         \\
    \abltablerow{apache/hadoop}                   {95}            {182}             {50}    {47}                           \\
    \abltablerow{apache/hbase}                  {80}            {91}             {24}       {20}                        \\
    \abltablerow{plume-lib/plume-util}          {4}                {11}             {3}         {2}                    \\
    \hline
    \abltablerow{\textbf{Total}}                {296}            {442}             {131}    {117}                           \\
  \end{tabularx}
\end{table}

Lightweight ownership (\cref{sec:lightweight-ownership}),
resource aliasing (\cref{sec:must-call-choice}), and
obligation creation (\cref{sec:reset-must-call})
reduce false positive warnings and improve \tool's precision.
To evaluate the contribution of each enhancement, we individually disabled each
feature and re-ran the experiments of \cref{sec:case-studies}.

\Cref{tab:ablation} shows that each of lightweight
ownership and resource aliases prevents more false positive errors than the total number
of remaining false positives on each benchmark.
The system for creating new obligations at points other than constructors reduces
false positives by a smaller amount: non-final, owning field re-assignments are rare,
and we encountered the unconnected socket pattern described in \cref{sec:unconnected-sockets}
only in ZooKeeper. Nevertheless, this feature allows our tool to handle an
important, if less common, coding pattern.

\subsection{Comparison to other tools}
\label{sec:compare}

Our approach represents a novel point in the design space of resource leak checkers.
%
This section compares our approach with two other modern tools that detect resource leaks:
\begin{itemize}
\item The analysis built into the Eclipse Compiler for Java (ecj), which is the default approach
  for detecting resource leaks in the Eclipse IDE~\cite{ecj-resource-leak}.
  We used version 4.18.0.
\item Grapple~\cite{zuo2019grapple} represents a significant, recent
  improvement in the scalability of typestate-based tools that require a whole-program alias analysis.
\end{itemize}
In brief, both tools are unsound.
Both tools are applicable to legacy code. % , without the need to write annotations.
Eclipse is very fast (nearly instantaneous) and suffers 67--98\% false
positives. % while missing 70-90\% of resource leaks.
Grapple is precise (13\% false positive rate
according to its authors; \todo{XX}\% false positive rate according to our
analysis of its output) but orders
of magnitude slower than \tool.
Different users can select whichever tool matches
their priorities.
\todo{Mike suggests mentioning our false positive rate for direct
  comparison in this section, and also including false negative rates
  directly.  We want to enable readers to make comparisons, not force them
  to hunt around the paper for information.}

\todo{This section should mention that both tools (I think?) neither
  require nor permit user-written specifications.  This is a plus in terms
  of ease of use, and a minus in terms of documentation, flexibility, and
  tool accuracy.}

Our comparison uses only the 3 case study programs that Grapple was run on
in the past; see \cref{sec:grapple} for details.

% Gets the (two-column) Grapple table on the same page as the reference to it.
\input{table-grapple.tex}


\subsubsection{Eclipse}
\label{sec:eclipse}

The Eclipse analysis is a simple dataflow analysis
augmented with heuristics. Since it is tightly integrated with
the compiler, it scales well and runs quickly. It has
heuristics for ownership, resource wrappers, and resource-free
closeables, among others; these are all hard-coded into the analysis and cannot
be adjusted by the user. It does not support annotations to express
specifications that differ from its defaults.
It supports two levels of analysis: detecting high-confidence resource
leaks and detecting ``potential'' resource
leaks (a superset of high-confidence resource leaks).\looseness=-1

We ran Eclipse's analysis on the same versions of our case study programs
that we ran \tool on in \cref{sec:case-studies}, and examined
the same modules. It is easy to apply
to legacy code and it is fast---nearly instantaneous once Eclipse
has loaded the project.\looseness=-1

In ``high-confidence'' mode on the three projects, Eclipse reports 9
warnings related to classes defined in the JDK:
2 true positives (thus, it misses 46 real resource leaks) and 7
false positives.
In ``potential'' leak mode, the analysis reports many more warnings.
Thus, we triaged only the 180
warnings about JDK classes from the ZooKeeper benchmark.
Among these were 3 true positives (it misses 10 real resource leaks) and 177 false
positives.
The most common cause of false
positives was the unchangeable, default ownership transfer assumption
at method invocations:
the potential leak mode warned at each call that returns a resource-alias, such as
\<Socket\#getInputStream>.

These results show that the Eclipse bug-finder is unsound in both
modes: it misses 10 real warnings on ZooKeeper even in potential leak mode.
The analysis is imprecise, too: in potential leak mode,
the vast majority of warnings are false positives, and even in high-confidence
mode its ratio of true to total warnings\todo{We should call this
  ``precision'' or at least add that word to the text.}
%2/9
(22\%)
is slightly higher\todo{I think this should be ``lower''.} than \tool's
% 48/166
(29\%).
%% MK: I think this sentence is redundant
%% However, this sort
%% of bug-finding tool does have two advantages: it is fast, and it is easily
%% applicable to legacy code.
\todo{Cut or move the remainder of this section; it is about qualitative
  differences.  Qualitative differences should appear earlier (or at least elsewhere), not in
  this paragraph about quantitative accuracy.}
Compared to our tool, the Eclipse analysis
is much faster and easier to apply to legacy code
(no annotations are required), but both unsound
and less precise.\looseness=-1

%% Chandra rightly pointed out that this para make us look lazy. Let's just
%% say what we did, not why we did it.
%%
%% Given that these results on zookeeper-server
%% clearly show the strengths and weaknesses of this tool compared to our own,
%% we did not run it on our other case study programs.


\subsubsection{Grapple}
\label{sec:grapple}

% \todo{Be very sure that Grapple is unsound - read their paper again, and
%    point out why in this paragraph.}
%\todo{be sure to note that Grapple is not modular, unlike us}
Grapple is a modern typestate-based resource leak analysis
``designed to conduct precise and scalable checking of finite-state properties for very
large codebases''~\cite{zuo2019grapple}. Grapple models its alias and
dataflow analyses as dynamic transitive-closure computations over graphs, and
it leverages novel path encodings and techniques from predecessor-system
Graspan~\cite{wang2017graspan} to achieve both context- and path-sensitivity.  
% Grapple, like all previous
% typestate-based analyses, requires a whole-program alias analysis.
Grapple contains four checkers, of which two are useful for detecting
resource leaks.  Unlike \tool, Grapple is unsound; e.g., it performs a fixed bounded unrolling
of loops to make path sensitivity tractable.\todo{Ask the Grapple authors if there are other reasons for
  unsoundness.}
\Tool reports violations of a user-supplied specification
(which takes effort to write but provides documentation benefits), so it
can ensure that a library is correct for all possible clients.  By
contrast, Grapple checks a library in the context of one specific client,
which might be the library's tests.

% \todo{Manu should update this paragraph with a response from the Grapple authors.}
% \todo{This paragraph feels too harsh to me.}
The Grapple authors evaluated their tool on earlier versions of the first three
case study programs in \cref{sec:case-studies}~\cite{zuo2019grapple}.
Unfortunately, a comparison on the same version is not possible, because
Grapple's leak detector currently cannot be run
% Mike feels this parenthesis is essential.  Otherwise, there is no support
% for the claim "cannot be run".  The fact that the Grapple authors cannot run it
% is essential information.
(by us or by the Grapple authors)
due to
\todo{clarify:  ``library incompatibilities'' or ``changed libraries'' or
  some other more descriptive phrase}incompatibilities and bitrot in the implementation.
The Grapple authors provided with
all warnings issued by Grapple in the
versions of the benchmarks they analyzed.

We used the following methodology to permit a head-to-head comparison.
We started with all the warning issues by either tool.
We disregarded any warning about code that is not present identically in the other
version of the target program.  (Code might not be present identically because of refactoring,
added code, bug fixes, etc.)  We also disregarded warnings about code that
is not checked by one of the tools.  For example, Grapple analyzed test
code, but our experiments did not write annotations in test code nor
type-check it.  The remaining warnings are
about resource leaks in identical code that both tools ought to report.
For each remaining warning, we manually identified it as a true positive (a
real resource leak) or a false positive (correct code, but the tool cannot
determine that fact).  \Cref{tab:tool-comparison} reports the precision
and recall of Eclipse, Grapple, and \tool.  Some of
Grapple's false positives are reports about types like
\<java.io.StringWriter> that have no underlying resource that needs to be
closed.
(This is a reason that the numbers in \cref{tab:tool-comparison} differ
from those reported in~\cite{zuo2019grapple}:  the Grapple authors had
mis-classified these warnings as true positives.)
Grapple's false negatives might be due to analysis unsoundness or gaps in API
modeling.\todo{Manu, give a concrete example: Grapple contains no FSM for
  OutputStream or some related class.}\todo{Ask the Grapple authors if there are other reasons for
  false negatives.}\todo{Is another reason that a routine is defective but the
  defective code is not reachable from \<main()>, which is how Grapple works?}
On the flip side, \tool has lower precision (more false positives)
than Grapple, and it requires programmer effort to annotate the source code.  

\todo{There needs to be an explicit discussion of whether Grapple
  permits/requires user-written specifications.}


\begin{table}
  \caption{Run times of resource leak checking tools.}
  \label{tab:run-times}
  \begin{tabular}{l|ccc}
    Project        & Eclipse & Grapple & Resource Leak Checker \\
    \hline
    ZooKeeper      & <5s & \zph 1h 07m 02s  & \zph 1m 24s  \\
    HDFS           & <5s & \zph 1h 54m 52s  &  16m 21s \\
    HBase          & <5s &     33h 51m 59s  & \zph 7m 45s  \\
  \end{tabular}
\end{table}

Grapple runs can take many hours (run times are from~\cite{zuo2019grapple}), whereas
\tool runs in minutes (\cref{tab:run-times}).
Further, to
our best knowledge\todo{Ask the authors explicitly, then remove ``to our
  best knowledge'' and add a citation to ``personal communication''.},
Grapple is not modular, so if the user edits their program, Grapple must be
re-run from scratch.  After a code edit,
\tool only needs to re-analyze modified code (and
possibly its dependents if the modified code's interface changed).

% LocalWords:  LoC SHAs leakable Closeable i7 GiB hadoop FileInputStream
% LocalWords:  getOffset ZooKeeper util apache hbase ecj getInputStream
% LocalWords:  codebases'' Graspan HBase HDFS
