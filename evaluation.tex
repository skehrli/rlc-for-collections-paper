\section{Evaluation}
\label{sec:evaluation}

% TODO: if this table is after tab:case-studies, then it doesn't appear? WTF

% a line in tab:ablation
% note that the FULL column is always zero, so it's not included here
% arguments: project name, NONE, LO-ONLY, AF-ONLY
\newcommand{\abltablerow}[4]{\textbf{\smaller{#1}} & #2 & #3 & #4 & 0}

\begin{table}
  \caption{An ablation study on the contribution of the lightweight
    ownership (\emph{NO-LO}), resource aliasing (\emph{NO-RA}),
    and accumulation frames (\emph{NO-AF})
    features in reducing false positives. Each entry is the number of extra
    false positive warnings reported by the variant with the given feature disabled on the given project.}
  \label{tab:ablation}
  
  \begin{tabularx}{\columnwidth}{@{}Xrrrr@{}}
    Project                              &      \emph{NO-LO} & \emph{NO-RA} & \emph{NO-AF} & \emph{FULL}      \\
    \hline
    \abltablerow{apache/zookeeper}              {66}            {97}             {10}                               \\
    \abltablerow{apache/hfds}                   {?}            {?}             {?}                               \\
    \abltablerow{apache/hbase}                  {?}            {?}             {?}                               \\
    \hline
    \abltablerow{\textbf{Total}}                {?}            {?}             {?}                               \\
  \end{tabularx}
\end{table}


% a line in tab:case-studies
% arguments: project name, original LoC, # of resources (-AcountMustCall), diff size, # of annotations, TPs, Confirmed TPs, FPs, run time in seconds
\newcommand{\osstablerow}[9]{\textbf{\smaller{#1}} & #2 & #3 & #4 & #5 & #6 & #7 & #8 & #9 s}

\begin{table*}
  \caption{Verifying the absence of resource leaks in case studies.
    Throughout, ``LoC'' is lines of non-comment, non-blank Java code.
    ``Resources'' is the number of resources created by the program.
    ``Diff size'' is the difference in LoC between the original and
    annotated programs, counting both annotations and modified code.
    ``Annos.'' is number of manually-written annotations to specify
    existing methods.
    ``TPs'' is true positives. ``Conf'' is confirmed true positives. 
    ``FPs'' is false positives, where the our analysis could not
  guarantee that the call was safe, but manual analysis revealed that no
  run-time failure was possible.
    ``RT(s)'' is the wall-clock run time of our analysis in seconds.}
  \label{tab:case-studies}

  \begin{tabular}{@{}lrr|rr|rrrr@{}}
    Project                              &      LoC      & Resources   &  Diff size  & Annos.   & TPs  & Conf    & FPs & RT(s)      \\
    \hline
    \osstablerow{apache/zookeeper:zookeeper-server}              {94,872}        {?}            {?}          {99}        {10}     {?}      {47}   {?}        \\
    \osstablerow{apache/hadoop:hdfs}                   {?}        {?}            {?}          {?}        {?}     {?}      {?}   {?}        \\
    \osstablerow{apache/hbase:?}                  {?}        {?}            {?}          {?}        {?}     {?}      {?}   {?}        \\
    \hline
    \osstablerow{\textbf{Total}}                {?}        {?}            {?}          {?}        {?}     {?}      {?}   {-}        \\
  \end{tabular}
\end{table*}


Our evaluation has three parts:
\begin{itemize}
\item case studies on open-source projects, which show that our approach
  scales to realistic programs and finds real bugs (\cref{sec:case-studies}).
\item an ablation study that demonstrates the contributions of
  lightweight ownership system (\cref{sec:lightweight-ownership}),
  resource aliasing (\cref{sec:must-call-choice}), and
  accumulation frames (\cref{sec:reset-must-call}) to the false positive
  rate on the case studies in \cref{sec:case-studies} (\cref{sec:ablation}).
\item a comparison study that shows the advantages of our approach against
  two traditional approaches: heuristic bug-finders and heavy-weight
  typestate analysis (\cref{sec:compare}).
\end{itemize}

\subsection{Case studies on open-source projects}
\label{sec:case-studies}

We selected \todo{3} popular open-source projects with significant
usage of resources (and thus high danger of resource leaks), by
convenience.
For each project, we selected and analyzed one module
containing significant uses of leakable resources.
We modified the build system of each project to run our
analysis. We manually annotated each program with must-call,
called-methods, and ownership annotations. We also
made small, semantics-preserving changes to the programs where
possible---such as adding
\<final> modifiers to fields---to avoid
false positives from our analysis. We then examined all
remaining warnings, and categorized them as either true
positives---real resource leaks---or false positives---code that our
system was insufficiently precise to prove correct. We submitted bug
reports and patches to the projects describing true positives, when time permitted.
We also measured the number of possible resource leaks in each project
and the run time of our analysis using \todo{a standardized machine similar to the one used
  in the Grapple paper}. Each case study took one author a few hours;
most of the time was spent understanding the code under analysis, which no
authors were familiar with before beginning, writing corresponding annotations
where appropriate, and reasoning through warnings emitted by the tool by hand
to determine whether a warning was a true or false positive.
The annotated versions of the case study programs are available at
\todo{an anonymized fork of the projects}.

\Cref{tab:case-studies} summarizes the results. \tool finds multiple
serious resource leak bugs in all of the examined programs. Though
there are more false positives than true positives in each program,
the number is small enough to be examined by a single developer in a
few hours---a small price to pay for knowing that the program is
definitely free of resource leaks.  The annotations in the program are
also a benefit: as a form of machine-checked documentation, they
express the programmer's intent and, unlike traditional comments,
cannot become out-of-date if the checker is passing.

\todo{Describe some examples of true and false positives.}

\todo{Discuss the annotations that had to be written by hand.}

\subsection{Ablating lightweight ownership, resource aliasing, and accumulation frames}
\label{sec:ablation}

Without lightweight ownership (\cref{sec:lightweight-ownership}),
resource aliasing (\cref{sec:must-call-choice}), and
accumulation frames (\cref{sec:reset-must-call}), our analysis produces significantly
more false positives. To show the contribution of each to our tool's precision,
we performed an ablation study on the same programs we used as case studies in
\cref{sec:case-studies} by building four versions of our tool:
one with lightweight ownership disabled (\emph{NO-LO}),
one with resource aliaisng disabled (\emph{NO-RA}),
and one with accumulation frames disabled(\emph{NO-AF}),
and the original with all three enabled (\emph{FULL}).
We re-ran the experiments in \cref{sec:case-studies} using each variant, and
recorded the change in the number of warnings. Since our analysis has no false
negatives, the difference between the total number of warnings (i.e. the sum of
the ``TPs'' and ``FPs'' columns in \cref{tab:case-studies}) produced by the \emph{FULL}
variant and the total number of false positives produced by each variant is
the number of false positives that are prevented by each feature.

\Cref{tab:ablation} has an entry for each variant on each case study project,
which is the number of extra warnings that that variant reported on the project,
compared to \emph{FULL}. From the table, it is clear that both lightweight
ownership and resource aliases are critically important to the precision of
\tool: each feature prevents more false positive errors than the total number
of remaining false positives on the Zookeeper benchmark, for example. \todo{Strengthen
  this claim when we have the results from the other benchmarks.} Though
the accumulation frame feature prevents fewer errors, it permits verification
of a complex pattern---the use of non-final, owning fields---that is used in a
few critical places in each benchmark.

\subsection{Comparison to other tools}
\label{sec:compare}

Our approach represents a novel point in the design space of a resource leak checker.

In this section, we compare our approach with two other modern tools that purport to detect resource leaks:
\begin{itemize}
\item the analysis built into the Eclipse Compiler for Java (ecj), which is the default approach
  for detecting resource leaks in the Eclipse IDE~\cite{ecj-resource-leak}.
\item Grapple~\cite{zuo2019grapple}, an unsound, typestate-based analysis that represents a significant, recent
  improvement in the scalability of typestate-based tools that require a whole-program alias analysis.
\end{itemize}
Each tool is evaluated against the four criteria that we proposed in \cref{sec:intro}
for an ideal resource-leak detector: analysis speed, applicability to legacy code, soundness (i.e. ability
to detect all errors), and precision (i.e. lack of false positive warnings).

\subsubsection{Eclipse}
\label{sec:eclipse}

The Eclipse analysis is a simple dataflow analysis
augmented with heuristics. Because it is tightly integrated with
the compiler, it scales well and runs quickly. It contains
heuristics for ownership, resource wrappers, and resource-free
closeables, among others; these are all hard-coded into the analysis and cannot
be adjusted by the user. It does not support annotations to express
specifications in these categories that differ from its defaults.
It supports two levels of analysis: detecting ``potential'' resource
leaks and detecting high-confidence resource leaks; the former reports a superset
of the warnings reported by the latter.

We ran this analysis on the same version of Zookeeper's zookeeper-server
module that we ran \tool on in \cref{sec:case-studies}. Once Eclipse
has loaded the project, the analysis runs nearly instantaneously---it
clearly satisfies the "fast" criterion. It is also easy to apply it to
existing legacy code---it clearly satisfies the "applicable" criterion.

In potential leak mode on zookeeper-server, the analysis reported 180
warnings.  Of these, we determined that 8 were true positives
(corresponding to true positives that our sound tool found), and the
other 172 were false positives.  The most common cause of false
positives was the unchangeable, default ownership transfer assumption
at method invocations: each call to a method such as
\<Socket\#getInputStream> that returns a resource-alias resulted in a
warning.  Of the true positives, 5 of 8 were warnings about such
calls, where the aliased resource really was not closed on all
paths. We believe that these true positives are "accidental"---the
analysis found them because of its imprecise handling of resource
aliases, not because it detected the actual leak. When the
``potential'' resource leaks are ignored in ``high-confidence'' mode,
Eclipse reports only 3 warnings: one true positive and two false
positives.

These results demonstrate that the Eclipse bug-finder is unsound in both
modes: it misses at least 4 real warnings that our tool found when detecting
potential leaks. Further, the analysis is imprecise: in potential leak mode,
the vast majority of warnings are false positives. However, this sort
of bug-finding tool does have two advantages: it is fast, and it is easily
applicable to legacy code. Compared to our tool, the Eclipse analysis
is much faster and equally applicable to legacy code, but both less sound
and less precise.

Given that these results on zookeeper-server
clearly show the strengths and weaknesses of this tool compared to our own,
we did not run it on our other case study programs.

\subsubsection{Grapple}
\label{sec:grapple}

% a line in tab:grapple
% these numbers come from tables 2 and 3 in the grapple paper from EuroSys.
% for TPs and FPs, I added together the IO and Socket columns of table 2.
\newcommand{\grappletablerow}[4]{\textbf{\smaller{#1}} & #2 & #3 & #4}

\begin{table}
  \caption{Key findings on the Grapple tool's performance on the case study
    programs in \cref{sec:case-studies}; reproduced from~\cite{zuo2019grapple}.}
  \label{tab:grapple}
  
  \begin{tabularx}{\columnwidth}{@{}Xrrr@{}}
    Project                              &  TPs    &    FPs         & Run time      \\
    \hline
    \grappletablerow{ZooKeeper}             {6}         {0}           {01h07m02s}     \\
    \grappletablerow{HDFS}                  {5}         {2}           {01h54m52s}    \\
    \grappletablerow{HBase}                 {15}        {2}           {33h51m59s}     \\
    \hline
    \grappletablerow{\textbf{Total}}        {26}        {4}           {-}          \\
  \end{tabularx}
\end{table}

\todo{be sure to note that Grapple is not modular, unlike us}
Grapple~\cite{zuo2019grapple} is a modern typestate-based resource leak analysis
focused on precision and (relative) scalability. Grapple models its alias and
dataflow analyses as dynamic transitive-closure computation of graph
representations of the target programs. Grapple, like all previous
typestate-based analyses, requires a whole-program alias analysis. Grapple
contains four checkers, of which two (IO and Socket) are useful for detecting
resource leaks.

The Grapple authors have already evaluated their tool on earlier
versions of the case study programs in
\cref{sec:case-studies}~\cite{zuo2019grapple}.  Unfortunately, we were
not able to run the tool on the same versions of the case study
programs that we used in \cref{sec:case-studies}.
\todo{Manu: explain why we couldn't/didn't try to do this.}
We reproduce their key findings below in \cref{tab:grapple}
(their run time numbers were computed on a commodity machine
\todo{similar to} the commodity machine we used to compute the run
times that appear in \cref{tab:case-studies}, but includes the cost
of running all four of their checkers).

Compared to our approach, Grapple is slower (due to its reliance on
an expensive whole-program alias analysis, which \tool avoids by using
an accumulation analysis), similarly applicable to legacy
code, less sound, and much more precise--our type-based approach
trades off precision and a small amount of user effort (i.e. to write annotations)
for soundness and speed. In this way, \tool and Grapple are complementary:
they optimize for different criteria. Users should select the tool whose
priorities match their own.


