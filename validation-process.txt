Here are the steps to take to validate the categorization of issues in our
benchmarks.

First, comment out all warning suppressions related to our checker.  This is a
bit tricky as suppressions may appear on multiple lines.  I believe that adding
`//` to the beginning of any line matching the regex
`.*"(mustcall|objectconstruction):` should work, but probably it's best to look
over the change and sanity check it.

Next, run the checker and collect the output in a text file.  Filter the output
using the appropriate script, and pipe the result to a new file: use
`errors-without-custom-types.sh` for Zookeeper, and
`warnings-without-custom-types.sh` for HBase or Hadoop (from the
`experimental-machinery/ablation` directory in the OCC repository).  This yields
the list of warnings that were categorized as true or false positives.

As you prefer, go through either the warning suppressions or the list of
warnings, and check the categorization of each warning as a true positive or
false positive.  If you agree with the categorization, write "(validated)" at
the end of the explanation.  If you disagree, write "(DISAGREE: reason)" at the
end of the explanation, giving your explanation for the disagreement.  When we
have converged, doing a search for this regex should return no results:

```
.*"(mustcall|objectconstruction):(?!.*\(validated\))
```

Watch out for cases where a single `@SuppressWarnings` annotation on a method
has been used to suppress multiple different warnings.  If you cannot separate
the single `@SuppressWarnings` into multiple suppressions on different
variables, and all the warnings have the same key, you can annotate the code by
duplicating the key, e.g.:

```
  @SuppressWarnings({
    "rawtypes", // FP: reason 1
    "rawtypes" // FP: reason 2
  })
```

